// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_DMA_MVCNN_H_
#define FLATBUFFERS_GENERATED_DMA_MVCNN_H_

#include "flatbuffers/flatbuffers.h"

#include "memoryManagement_generated.h"

namespace MVCNN {

struct NNDMATask;

struct UPADMATask;

struct NNDMATask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SRC = 4,
    VT_DST = 6,
    VT_COMPRESSION = 8
  };
  const TensorReference *src() const {
    return GetPointer<const TensorReference *>(VT_SRC);
  }
  const flatbuffers::Vector<flatbuffers::Offset<TensorReference>> *dst() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<TensorReference>> *>(VT_DST);
  }
  bool compression() const {
    return GetField<uint8_t>(VT_COMPRESSION, 0) != 0;
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SRC) &&
           verifier.VerifyTable(src()) &&
           VerifyOffset(verifier, VT_DST) &&
           verifier.VerifyVector(dst()) &&
           verifier.VerifyVectorOfTables(dst()) &&
           VerifyField<uint8_t>(verifier, VT_COMPRESSION) &&
           verifier.EndTable();
  }
};

struct NNDMATaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_src(flatbuffers::Offset<TensorReference> src) {
    fbb_.AddOffset(NNDMATask::VT_SRC, src);
  }
  void add_dst(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<TensorReference>>> dst) {
    fbb_.AddOffset(NNDMATask::VT_DST, dst);
  }
  void add_compression(bool compression) {
    fbb_.AddElement<uint8_t>(NNDMATask::VT_COMPRESSION, static_cast<uint8_t>(compression), 0);
  }
  explicit NNDMATaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NNDMATaskBuilder &operator=(const NNDMATaskBuilder &);
  flatbuffers::Offset<NNDMATask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NNDMATask>(end);
    return o;
  }
};

inline flatbuffers::Offset<NNDMATask> CreateNNDMATask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> src = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<TensorReference>>> dst = 0,
    bool compression = false) {
  NNDMATaskBuilder builder_(_fbb);
  builder_.add_dst(dst);
  builder_.add_src(src);
  builder_.add_compression(compression);
  return builder_.Finish();
}

inline flatbuffers::Offset<NNDMATask> CreateNNDMATaskDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> src = 0,
    const std::vector<flatbuffers::Offset<TensorReference>> *dst = nullptr,
    bool compression = false) {
  auto dst__ = dst ? _fbb.CreateVector<flatbuffers::Offset<TensorReference>>(*dst) : 0;
  return MVCNN::CreateNNDMATask(
      _fbb,
      src,
      dst__,
      compression);
}

struct UPADMATask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SRC = 4,
    VT_DST = 6
  };
  const TensorReference *src() const {
    return GetPointer<const TensorReference *>(VT_SRC);
  }
  const TensorReference *dst() const {
    return GetPointer<const TensorReference *>(VT_DST);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SRC) &&
           verifier.VerifyTable(src()) &&
           VerifyOffset(verifier, VT_DST) &&
           verifier.VerifyTable(dst()) &&
           verifier.EndTable();
  }
};

struct UPADMATaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_src(flatbuffers::Offset<TensorReference> src) {
    fbb_.AddOffset(UPADMATask::VT_SRC, src);
  }
  void add_dst(flatbuffers::Offset<TensorReference> dst) {
    fbb_.AddOffset(UPADMATask::VT_DST, dst);
  }
  explicit UPADMATaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  UPADMATaskBuilder &operator=(const UPADMATaskBuilder &);
  flatbuffers::Offset<UPADMATask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<UPADMATask>(end);
    return o;
  }
};

inline flatbuffers::Offset<UPADMATask> CreateUPADMATask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> src = 0,
    flatbuffers::Offset<TensorReference> dst = 0) {
  UPADMATaskBuilder builder_(_fbb);
  builder_.add_dst(dst);
  builder_.add_src(src);
  return builder_.Finish();
}

}  // namespace MVCNN

#endif  // FLATBUFFERS_GENERATED_DMA_MVCNN_H_
