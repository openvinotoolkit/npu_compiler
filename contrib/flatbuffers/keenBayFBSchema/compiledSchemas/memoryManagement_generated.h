// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_MEMORYMANAGEMENT_MVCNN_H_
#define FLATBUFFERS_GENERATED_MEMORYMANAGEMENT_MVCNN_H_

#include "flatbuffers/flatbuffers.h"

namespace MVCNN {

struct BinaryData;

struct IndirectDataReference;

struct TensorReference;

enum MemoryLocation {
  /// Values indicating which type of memory a tensor resides in
  ///
  MemoryLocation_NULL = 0,
  MemoryLocation_ProgrammableInput = 1,
  MemoryLocation_ProgrammableOutput = 2,
  MemoryLocation_VPU_DDR_Heap = 3,
  MemoryLocation_GraphFile = 4,
  MemoryLocation_VPU_CMX_NN = 5,
  MemoryLocation_VPU_CMX_UPA = 6,
  MemoryLocation_VPU_DDR_BSS = 7,
  MemoryLocation_MIN = MemoryLocation_NULL,
  MemoryLocation_MAX = MemoryLocation_VPU_DDR_BSS
};

inline const MemoryLocation (&EnumValuesMemoryLocation())[8] {
  static const MemoryLocation values[] = {
    MemoryLocation_NULL,
    MemoryLocation_ProgrammableInput,
    MemoryLocation_ProgrammableOutput,
    MemoryLocation_VPU_DDR_Heap,
    MemoryLocation_GraphFile,
    MemoryLocation_VPU_CMX_NN,
    MemoryLocation_VPU_CMX_UPA,
    MemoryLocation_VPU_DDR_BSS
  };
  return values;
}

inline const char * const *EnumNamesMemoryLocation() {
  static const char * const names[] = {
    "NULL",
    "ProgrammableInput",
    "ProgrammableOutput",
    "VPU_DDR_Heap",
    "GraphFile",
    "VPU_CMX_NN",
    "VPU_CMX_UPA",
    "VPU_DDR_BSS",
    nullptr
  };
  return names;
}

inline const char *EnumNameMemoryLocation(MemoryLocation e) {
  if (e < MemoryLocation_NULL || e > MemoryLocation_VPU_DDR_BSS) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesMemoryLocation()[index];
}

enum DType {
  /// An enum to be used by a TensorReference Object, so that
  /// it will know how to access data from its buffer.
  DType_NOT_SET = 0,
  DType_FP64 = 1,
  DType_FP32 = 2,
  DType_FP16 = 3,
  DType_FP8 = 4,
  DType_U64 = 5,
  DType_U32 = 6,
  DType_U16 = 7,
  DType_U8 = 8,
  DType_I64 = 9,
  DType_I32 = 10,
  DType_I16 = 11,
  DType_I8 = 12,
  DType_I4 = 13,
  DType_I2 = 14,
  DType_I4X = 15,
  DType_BIN = 16,
  DType_LOG = 17,
  DType_I2X = 18,
  DType_MIN = DType_NOT_SET,
  DType_MAX = DType_I2X
};

inline const DType (&EnumValuesDType())[19] {
  static const DType values[] = {
    DType_NOT_SET,
    DType_FP64,
    DType_FP32,
    DType_FP16,
    DType_FP8,
    DType_U64,
    DType_U32,
    DType_U16,
    DType_U8,
    DType_I64,
    DType_I32,
    DType_I16,
    DType_I8,
    DType_I4,
    DType_I2,
    DType_I4X,
    DType_BIN,
    DType_LOG,
    DType_I2X
  };
  return values;
}

inline const char * const *EnumNamesDType() {
  static const char * const names[] = {
    "NOT_SET",
    "FP64",
    "FP32",
    "FP16",
    "FP8",
    "U64",
    "U32",
    "U16",
    "U8",
    "I64",
    "I32",
    "I16",
    "I8",
    "I4",
    "I2",
    "I4X",
    "BIN",
    "LOG",
    "I2X",
    nullptr
  };
  return names;
}

inline const char *EnumNameDType(DType e) {
  if (e < DType_NOT_SET || e > DType_I2X) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesDType()[index];
}

struct BinaryData FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FP64 = 4,
    VT_FP32 = 6,
    VT_FP16 = 8,
    VT_F8 = 10,
    VT_U64 = 12,
    VT_U32 = 14,
    VT_U16 = 16,
    VT_U8 = 18,
    VT_I64 = 20,
    VT_I32 = 22,
    VT_I16 = 24,
    VT_I8 = 26,
    VT_I4 = 28,
    VT_I2 = 30,
    VT_I2X = 32,
    VT_I4X = 34,
    VT_BIN = 36,
    VT_LOG = 38
  };
  const flatbuffers::Vector<double> *fp64() const {
    return GetPointer<const flatbuffers::Vector<double> *>(VT_FP64);
  }
  const flatbuffers::Vector<float> *fp32() const {
    return GetPointer<const flatbuffers::Vector<float> *>(VT_FP32);
  }
  const flatbuffers::Vector<int16_t> *fp16() const {
    return GetPointer<const flatbuffers::Vector<int16_t> *>(VT_FP16);
  }
  const flatbuffers::Vector<uint8_t> *f8() const {
    return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_F8);
  }
  const flatbuffers::Vector<uint64_t> *u64() const {
    return GetPointer<const flatbuffers::Vector<uint64_t> *>(VT_U64);
  }
  const flatbuffers::Vector<uint32_t> *u32() const {
    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_U32);
  }
  const flatbuffers::Vector<uint16_t> *u16() const {
    return GetPointer<const flatbuffers::Vector<uint16_t> *>(VT_U16);
  }
  const flatbuffers::Vector<uint8_t> *u8() const {
    return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_U8);
  }
  const flatbuffers::Vector<uint64_t> *i64() const {
    return GetPointer<const flatbuffers::Vector<uint64_t> *>(VT_I64);
  }
  const flatbuffers::Vector<int32_t> *i32() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_I32);
  }
  const flatbuffers::Vector<int16_t> *i16() const {
    return GetPointer<const flatbuffers::Vector<int16_t> *>(VT_I16);
  }
  const flatbuffers::Vector<int8_t> *i8() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_I8);
  }
  const flatbuffers::Vector<int8_t> *i4() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_I4);
  }
  const flatbuffers::Vector<int8_t> *i2() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_I2);
  }
  const flatbuffers::Vector<int8_t> *i2x() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_I2X);
  }
  const flatbuffers::Vector<int8_t> *i4x() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_I4X);
  }
  const flatbuffers::Vector<int8_t> *bin() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_BIN);
  }
  const flatbuffers::Vector<int8_t> *log() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_LOG);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_FP64) &&
           verifier.VerifyVector(fp64()) &&
           VerifyOffset(verifier, VT_FP32) &&
           verifier.VerifyVector(fp32()) &&
           VerifyOffset(verifier, VT_FP16) &&
           verifier.VerifyVector(fp16()) &&
           VerifyOffset(verifier, VT_F8) &&
           verifier.VerifyVector(f8()) &&
           VerifyOffset(verifier, VT_U64) &&
           verifier.VerifyVector(u64()) &&
           VerifyOffset(verifier, VT_U32) &&
           verifier.VerifyVector(u32()) &&
           VerifyOffset(verifier, VT_U16) &&
           verifier.VerifyVector(u16()) &&
           VerifyOffset(verifier, VT_U8) &&
           verifier.VerifyVector(u8()) &&
           VerifyOffset(verifier, VT_I64) &&
           verifier.VerifyVector(i64()) &&
           VerifyOffset(verifier, VT_I32) &&
           verifier.VerifyVector(i32()) &&
           VerifyOffset(verifier, VT_I16) &&
           verifier.VerifyVector(i16()) &&
           VerifyOffset(verifier, VT_I8) &&
           verifier.VerifyVector(i8()) &&
           VerifyOffset(verifier, VT_I4) &&
           verifier.VerifyVector(i4()) &&
           VerifyOffset(verifier, VT_I2) &&
           verifier.VerifyVector(i2()) &&
           VerifyOffset(verifier, VT_I2X) &&
           verifier.VerifyVector(i2x()) &&
           VerifyOffset(verifier, VT_I4X) &&
           verifier.VerifyVector(i4x()) &&
           VerifyOffset(verifier, VT_BIN) &&
           verifier.VerifyVector(bin()) &&
           VerifyOffset(verifier, VT_LOG) &&
           verifier.VerifyVector(log()) &&
           verifier.EndTable();
  }
};

struct BinaryDataBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_fp64(flatbuffers::Offset<flatbuffers::Vector<double>> fp64) {
    fbb_.AddOffset(BinaryData::VT_FP64, fp64);
  }
  void add_fp32(flatbuffers::Offset<flatbuffers::Vector<float>> fp32) {
    fbb_.AddOffset(BinaryData::VT_FP32, fp32);
  }
  void add_fp16(flatbuffers::Offset<flatbuffers::Vector<int16_t>> fp16) {
    fbb_.AddOffset(BinaryData::VT_FP16, fp16);
  }
  void add_f8(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> f8) {
    fbb_.AddOffset(BinaryData::VT_F8, f8);
  }
  void add_u64(flatbuffers::Offset<flatbuffers::Vector<uint64_t>> u64) {
    fbb_.AddOffset(BinaryData::VT_U64, u64);
  }
  void add_u32(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> u32) {
    fbb_.AddOffset(BinaryData::VT_U32, u32);
  }
  void add_u16(flatbuffers::Offset<flatbuffers::Vector<uint16_t>> u16) {
    fbb_.AddOffset(BinaryData::VT_U16, u16);
  }
  void add_u8(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> u8) {
    fbb_.AddOffset(BinaryData::VT_U8, u8);
  }
  void add_i64(flatbuffers::Offset<flatbuffers::Vector<uint64_t>> i64) {
    fbb_.AddOffset(BinaryData::VT_I64, i64);
  }
  void add_i32(flatbuffers::Offset<flatbuffers::Vector<int32_t>> i32) {
    fbb_.AddOffset(BinaryData::VT_I32, i32);
  }
  void add_i16(flatbuffers::Offset<flatbuffers::Vector<int16_t>> i16) {
    fbb_.AddOffset(BinaryData::VT_I16, i16);
  }
  void add_i8(flatbuffers::Offset<flatbuffers::Vector<int8_t>> i8) {
    fbb_.AddOffset(BinaryData::VT_I8, i8);
  }
  void add_i4(flatbuffers::Offset<flatbuffers::Vector<int8_t>> i4) {
    fbb_.AddOffset(BinaryData::VT_I4, i4);
  }
  void add_i2(flatbuffers::Offset<flatbuffers::Vector<int8_t>> i2) {
    fbb_.AddOffset(BinaryData::VT_I2, i2);
  }
  void add_i2x(flatbuffers::Offset<flatbuffers::Vector<int8_t>> i2x) {
    fbb_.AddOffset(BinaryData::VT_I2X, i2x);
  }
  void add_i4x(flatbuffers::Offset<flatbuffers::Vector<int8_t>> i4x) {
    fbb_.AddOffset(BinaryData::VT_I4X, i4x);
  }
  void add_bin(flatbuffers::Offset<flatbuffers::Vector<int8_t>> bin) {
    fbb_.AddOffset(BinaryData::VT_BIN, bin);
  }
  void add_log(flatbuffers::Offset<flatbuffers::Vector<int8_t>> log) {
    fbb_.AddOffset(BinaryData::VT_LOG, log);
  }
  explicit BinaryDataBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  BinaryDataBuilder &operator=(const BinaryDataBuilder &);
  flatbuffers::Offset<BinaryData> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<BinaryData>(end);
    return o;
  }
};

inline flatbuffers::Offset<BinaryData> CreateBinaryData(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<double>> fp64 = 0,
    flatbuffers::Offset<flatbuffers::Vector<float>> fp32 = 0,
    flatbuffers::Offset<flatbuffers::Vector<int16_t>> fp16 = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint8_t>> f8 = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint64_t>> u64 = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> u32 = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint16_t>> u16 = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint8_t>> u8 = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint64_t>> i64 = 0,
    flatbuffers::Offset<flatbuffers::Vector<int32_t>> i32 = 0,
    flatbuffers::Offset<flatbuffers::Vector<int16_t>> i16 = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> i8 = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> i4 = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> i2 = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> i2x = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> i4x = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> bin = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> log = 0) {
  BinaryDataBuilder builder_(_fbb);
  builder_.add_log(log);
  builder_.add_bin(bin);
  builder_.add_i4x(i4x);
  builder_.add_i2x(i2x);
  builder_.add_i2(i2);
  builder_.add_i4(i4);
  builder_.add_i8(i8);
  builder_.add_i16(i16);
  builder_.add_i32(i32);
  builder_.add_i64(i64);
  builder_.add_u8(u8);
  builder_.add_u16(u16);
  builder_.add_u32(u32);
  builder_.add_u64(u64);
  builder_.add_f8(f8);
  builder_.add_fp16(fp16);
  builder_.add_fp32(fp32);
  builder_.add_fp64(fp64);
  return builder_.Finish();
}

inline flatbuffers::Offset<BinaryData> CreateBinaryDataDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<double> *fp64 = nullptr,
    const std::vector<float> *fp32 = nullptr,
    const std::vector<int16_t> *fp16 = nullptr,
    const std::vector<uint8_t> *f8 = nullptr,
    const std::vector<uint64_t> *u64 = nullptr,
    const std::vector<uint32_t> *u32 = nullptr,
    const std::vector<uint16_t> *u16 = nullptr,
    const std::vector<uint8_t> *u8 = nullptr,
    const std::vector<uint64_t> *i64 = nullptr,
    const std::vector<int32_t> *i32 = nullptr,
    const std::vector<int16_t> *i16 = nullptr,
    const std::vector<int8_t> *i8 = nullptr,
    const std::vector<int8_t> *i4 = nullptr,
    const std::vector<int8_t> *i2 = nullptr,
    const std::vector<int8_t> *i2x = nullptr,
    const std::vector<int8_t> *i4x = nullptr,
    const std::vector<int8_t> *bin = nullptr,
    const std::vector<int8_t> *log = nullptr) {
  auto fp64__ = fp64 ? _fbb.CreateVector<double>(*fp64) : 0;
  auto fp32__ = fp32 ? _fbb.CreateVector<float>(*fp32) : 0;
  auto fp16__ = fp16 ? _fbb.CreateVector<int16_t>(*fp16) : 0;
  auto f8__ = f8 ? _fbb.CreateVector<uint8_t>(*f8) : 0;
  auto u64__ = u64 ? _fbb.CreateVector<uint64_t>(*u64) : 0;
  auto u32__ = u32 ? _fbb.CreateVector<uint32_t>(*u32) : 0;
  auto u16__ = u16 ? _fbb.CreateVector<uint16_t>(*u16) : 0;
  auto u8__ = u8 ? _fbb.CreateVector<uint8_t>(*u8) : 0;
  auto i64__ = i64 ? _fbb.CreateVector<uint64_t>(*i64) : 0;
  auto i32__ = i32 ? _fbb.CreateVector<int32_t>(*i32) : 0;
  auto i16__ = i16 ? _fbb.CreateVector<int16_t>(*i16) : 0;
  auto i8__ = i8 ? _fbb.CreateVector<int8_t>(*i8) : 0;
  auto i4__ = i4 ? _fbb.CreateVector<int8_t>(*i4) : 0;
  auto i2__ = i2 ? _fbb.CreateVector<int8_t>(*i2) : 0;
  auto i2x__ = i2x ? _fbb.CreateVector<int8_t>(*i2x) : 0;
  auto i4x__ = i4x ? _fbb.CreateVector<int8_t>(*i4x) : 0;
  auto bin__ = bin ? _fbb.CreateVector<int8_t>(*bin) : 0;
  auto log__ = log ? _fbb.CreateVector<int8_t>(*log) : 0;
  return MVCNN::CreateBinaryData(
      _fbb,
      fp64__,
      fp32__,
      fp16__,
      f8__,
      u64__,
      u32__,
      u16__,
      u8__,
      i64__,
      i32__,
      i16__,
      i8__,
      i4__,
      i2__,
      i2x__,
      i4x__,
      bin__,
      log__);
}

struct IndirectDataReference FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_INDEX = 4,
    VT_SPARSITY_INDEX = 6
  };
  /// Index/Offsets from the start of a memory location (see MemoryLocation)
  ///
  /// There are two different access patterns.
  /// A) Array Indexing
  ///    This pattern is only currently used for the MemoryLocation "GraphFile". As the binary data in
  ///    flatbuffers is stored as a vector of vectors (a vector of tensor data), we can index them where
  ///    a value of N will access the Nth element of the vector, i.e. the Nth stored tensor data
  /// B) Byte Offset
  ///    For device buffers, most of the time we are dealing with a starting pointer and a total size.
  ///    (e.g. UPA CMX starts at 0x7000000 and is 128Kb [not verified values])
  ///    A value of N will access StartAddress + N
  ///
  uint32_t data_index() const {
    return GetField<uint32_t>(VT_DATA_INDEX, 0);
  }
  uint32_t sparsity_index() const {
    return GetField<uint32_t>(VT_SPARSITY_INDEX, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_DATA_INDEX) &&
           VerifyField<uint32_t>(verifier, VT_SPARSITY_INDEX) &&
           verifier.EndTable();
  }
};

struct IndirectDataReferenceBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_data_index(uint32_t data_index) {
    fbb_.AddElement<uint32_t>(IndirectDataReference::VT_DATA_INDEX, data_index, 0);
  }
  void add_sparsity_index(uint32_t sparsity_index) {
    fbb_.AddElement<uint32_t>(IndirectDataReference::VT_SPARSITY_INDEX, sparsity_index, 0);
  }
  explicit IndirectDataReferenceBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  IndirectDataReferenceBuilder &operator=(const IndirectDataReferenceBuilder &);
  flatbuffers::Offset<IndirectDataReference> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<IndirectDataReference>(end);
    return o;
  }
};

inline flatbuffers::Offset<IndirectDataReference> CreateIndirectDataReference(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t data_index = 0,
    uint32_t sparsity_index = 0) {
  IndirectDataReferenceBuilder builder_(_fbb);
  builder_.add_sparsity_index(sparsity_index);
  builder_.add_data_index(data_index);
  return builder_.Finish();
}

struct TensorReference FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMENSIONS = 4,
    VT_STRIDES = 6,
    VT_LEADING_OFFSET = 8,
    VT_TRAILING_OFFSET = 10,
    VT_DATA = 12,
    VT_LOCALE = 14,
    VT_DATA_DTYPE = 16,
    VT_QUANT_SCALE = 18,
    VT_QUANT_ZERO = 20,
    VT_QUANT_SHIFT = 22
  };
  /// Information on how to access a Tensor
  const flatbuffers::Vector<uint32_t> *dimensions() const {
    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_DIMENSIONS);
  }
  const flatbuffers::Vector<uint32_t> *strides() const {
    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_STRIDES);
  }
  uint32_t leading_offset() const {
    return GetField<uint32_t>(VT_LEADING_OFFSET, 0);
  }
  uint32_t trailing_offset() const {
    return GetField<uint32_t>(VT_TRAILING_OFFSET, 0);
  }
  const IndirectDataReference *data() const {
    return GetPointer<const IndirectDataReference *>(VT_DATA);
  }
  MemoryLocation locale() const {
    return static_cast<MemoryLocation>(GetField<int8_t>(VT_LOCALE, 0));
  }
  DType data_dtype() const {
    return static_cast<DType>(GetField<int8_t>(VT_DATA_DTYPE, 0));
  }
  const flatbuffers::Vector<int8_t> *quant_scale() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_QUANT_SCALE);
  }
  const flatbuffers::Vector<int8_t> *quant_zero() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_QUANT_ZERO);
  }
  const flatbuffers::Vector<int8_t> *quant_shift() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_QUANT_SHIFT);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMENSIONS) &&
           verifier.VerifyVector(dimensions()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<uint32_t>(verifier, VT_LEADING_OFFSET) &&
           VerifyField<uint32_t>(verifier, VT_TRAILING_OFFSET) &&
           VerifyOffset(verifier, VT_DATA) &&
           verifier.VerifyTable(data()) &&
           VerifyField<int8_t>(verifier, VT_LOCALE) &&
           VerifyField<int8_t>(verifier, VT_DATA_DTYPE) &&
           VerifyOffset(verifier, VT_QUANT_SCALE) &&
           verifier.VerifyVector(quant_scale()) &&
           VerifyOffset(verifier, VT_QUANT_ZERO) &&
           verifier.VerifyVector(quant_zero()) &&
           VerifyOffset(verifier, VT_QUANT_SHIFT) &&
           verifier.VerifyVector(quant_shift()) &&
           verifier.EndTable();
  }
};

struct TensorReferenceBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_dimensions(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> dimensions) {
    fbb_.AddOffset(TensorReference::VT_DIMENSIONS, dimensions);
  }
  void add_strides(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> strides) {
    fbb_.AddOffset(TensorReference::VT_STRIDES, strides);
  }
  void add_leading_offset(uint32_t leading_offset) {
    fbb_.AddElement<uint32_t>(TensorReference::VT_LEADING_OFFSET, leading_offset, 0);
  }
  void add_trailing_offset(uint32_t trailing_offset) {
    fbb_.AddElement<uint32_t>(TensorReference::VT_TRAILING_OFFSET, trailing_offset, 0);
  }
  void add_data(flatbuffers::Offset<IndirectDataReference> data) {
    fbb_.AddOffset(TensorReference::VT_DATA, data);
  }
  void add_locale(MemoryLocation locale) {
    fbb_.AddElement<int8_t>(TensorReference::VT_LOCALE, static_cast<int8_t>(locale), 0);
  }
  void add_data_dtype(DType data_dtype) {
    fbb_.AddElement<int8_t>(TensorReference::VT_DATA_DTYPE, static_cast<int8_t>(data_dtype), 0);
  }
  void add_quant_scale(flatbuffers::Offset<flatbuffers::Vector<int8_t>> quant_scale) {
    fbb_.AddOffset(TensorReference::VT_QUANT_SCALE, quant_scale);
  }
  void add_quant_zero(flatbuffers::Offset<flatbuffers::Vector<int8_t>> quant_zero) {
    fbb_.AddOffset(TensorReference::VT_QUANT_ZERO, quant_zero);
  }
  void add_quant_shift(flatbuffers::Offset<flatbuffers::Vector<int8_t>> quant_shift) {
    fbb_.AddOffset(TensorReference::VT_QUANT_SHIFT, quant_shift);
  }
  explicit TensorReferenceBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  TensorReferenceBuilder &operator=(const TensorReferenceBuilder &);
  flatbuffers::Offset<TensorReference> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<TensorReference>(end);
    return o;
  }
};

inline flatbuffers::Offset<TensorReference> CreateTensorReference(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> dimensions = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> strides = 0,
    uint32_t leading_offset = 0,
    uint32_t trailing_offset = 0,
    flatbuffers::Offset<IndirectDataReference> data = 0,
    MemoryLocation locale = MemoryLocation_NULL,
    DType data_dtype = DType_NOT_SET,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> quant_scale = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> quant_zero = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> quant_shift = 0) {
  TensorReferenceBuilder builder_(_fbb);
  builder_.add_quant_shift(quant_shift);
  builder_.add_quant_zero(quant_zero);
  builder_.add_quant_scale(quant_scale);
  builder_.add_data(data);
  builder_.add_trailing_offset(trailing_offset);
  builder_.add_leading_offset(leading_offset);
  builder_.add_strides(strides);
  builder_.add_dimensions(dimensions);
  builder_.add_data_dtype(data_dtype);
  builder_.add_locale(locale);
  return builder_.Finish();
}

inline flatbuffers::Offset<TensorReference> CreateTensorReferenceDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint32_t> *dimensions = nullptr,
    const std::vector<uint32_t> *strides = nullptr,
    uint32_t leading_offset = 0,
    uint32_t trailing_offset = 0,
    flatbuffers::Offset<IndirectDataReference> data = 0,
    MemoryLocation locale = MemoryLocation_NULL,
    DType data_dtype = DType_NOT_SET,
    const std::vector<int8_t> *quant_scale = nullptr,
    const std::vector<int8_t> *quant_zero = nullptr,
    const std::vector<int8_t> *quant_shift = nullptr) {
  auto dimensions__ = dimensions ? _fbb.CreateVector<uint32_t>(*dimensions) : 0;
  auto strides__ = strides ? _fbb.CreateVector<uint32_t>(*strides) : 0;
  auto quant_scale__ = quant_scale ? _fbb.CreateVector<int8_t>(*quant_scale) : 0;
  auto quant_zero__ = quant_zero ? _fbb.CreateVector<int8_t>(*quant_zero) : 0;
  auto quant_shift__ = quant_shift ? _fbb.CreateVector<int8_t>(*quant_shift) : 0;
  return MVCNN::CreateTensorReference(
      _fbb,
      dimensions__,
      strides__,
      leading_offset,
      trailing_offset,
      data,
      locale,
      data_dtype,
      quant_scale__,
      quant_zero__,
      quant_shift__);
}

}  // namespace MVCNN

#endif  // FLATBUFFERS_GENERATED_MEMORYMANAGEMENT_MVCNN_H_
