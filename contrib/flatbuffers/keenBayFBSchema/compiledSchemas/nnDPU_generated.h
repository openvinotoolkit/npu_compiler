// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_NNDPU_MVCNN_H_
#define FLATBUFFERS_GENERATED_NNDPU_MVCNN_H_

#include "flatbuffers/flatbuffers.h"

#include "memoryManagement_generated.h"

namespace MVCNN {

struct PPEGenericTask;

struct DPUInvariantFields;

struct DPUVariantFields;

struct DPUTask;

enum DPULayerType {
  DPULayerType_CONV = 0,
  DPULayerType_DWCONV = 1,
  DPULayerType_MAXPOOL = 2,
  DPULayerType_AVEPOOL = 3,
  DPULayerType_FCL = 4,
  DPULayerType_ELTWISE = 5,
  DPULayerType_MIN = DPULayerType_CONV,
  DPULayerType_MAX = DPULayerType_ELTWISE
};

inline const DPULayerType (&EnumValuesDPULayerType())[6] {
  static const DPULayerType values[] = {
    DPULayerType_CONV,
    DPULayerType_DWCONV,
    DPULayerType_MAXPOOL,
    DPULayerType_AVEPOOL,
    DPULayerType_FCL,
    DPULayerType_ELTWISE
  };
  return values;
}

inline const char * const *EnumNamesDPULayerType() {
  static const char * const names[] = {
    "CONV",
    "DWCONV",
    "MAXPOOL",
    "AVEPOOL",
    "FCL",
    "ELTWISE",
    nullptr
  };
  return names;
}

inline const char *EnumNameDPULayerType(DPULayerType e) {
  if (e < DPULayerType_CONV || e > DPULayerType_ELTWISE) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesDPULayerType()[index];
}

enum PPELayerType {
  /// Overview:
  /// The possible operations that the PPE unit can perform after
  /// a DPU operation
  /// Fields based on details from 14_08_NCE_PPE.odt
  /// Excluded Layer: BYPASS - If an array of PPELayers is empty, the device should know to run BYPASS
  ///  --- Low-Level Instructions ---
  /// These instructions are only for advanced usage.
  /// Stores register value to memory
  PPELayerType_STORE = 0  /// Loads a register (2 clock cycles)
,
  PPELayerType_LOAD = 1  /// Clears a register
,
  PPELayerType_CLEAR = 2  /// No Operation - Used for delaying (in cases like LOAD).
,
  PPELayerType_NOOP = 3  /// Stops the PPE.
,
  PPELayerType_HALT = 4  ///  --- Element-wise Operations ---
  /// Sum of 2 operands
,
  PPELayerType_ADD = 5  /// Subtraction of 2 operands
,
  PPELayerType_SUB = 6  /// Multiplication of 2 operands
,
  PPELayerType_MULT = 7  ///  --- Rectification Unit Variants ---
,
  PPELayerType_LRELU = 8,
  PPELayerType_LRELUX = 9,
  PPELayerType_LPRELU = 10  ///  --- Threholding & Limits ---
  /// Maximum of two operands
,
  PPELayerType_MAXIMUM = 11  /// Minimum of two operands
,
  PPELayerType_MINIMUM = 12  /// Ceiling of one operand
,
  PPELayerType_CEIL = 13  /// Floor of one operand
,
  PPELayerType_FLOOR = 14  ///  --- Bitwise Operations ---
  /// Bitwise AND of 2 operations
,
  PPELayerType_AND = 15  /// Bitwise OR of 2 operations
,
  PPELayerType_OR = 16  /// Bitwise XOR of 2 operations
,
  PPELayerType_XOR = 17  /// Bitwise NOT of 1 operations
,
  PPELayerType_NOT = 18  /// Bitwise ABS of 1 operations (Signed Only)
,
  PPELayerType_ABS = 19  /// Bitwise NEG of 1 operations (Signed Only)
,
  PPELayerType_NEG = 20  ///  --- Math Operations (i13 scaling required) ---
  /// X^N
,
  PPELayerType_POW = 21  /// Exp(X)
,
  PPELayerType_EXP = 22  /// Sigmoid(X)
,
  PPELayerType_SIGMOID = 23  /// TanH(X)
,
  PPELayerType_TANH = 24  /// SquareRoot(X)
,
  PPELayerType_SQRT = 25  /// 1/SquareRoot(X)
,
  PPELayerType_RSQRT = 26  /// Programmable Math Function
,
  PPELayerType_FLEXARB = 27,
  PPELayerType_MIN = PPELayerType_STORE,
  PPELayerType_MAX = PPELayerType_FLEXARB
};

inline const PPELayerType (&EnumValuesPPELayerType())[28] {
  static const PPELayerType values[] = {
    PPELayerType_STORE,
    PPELayerType_LOAD,
    PPELayerType_CLEAR,
    PPELayerType_NOOP,
    PPELayerType_HALT,
    PPELayerType_ADD,
    PPELayerType_SUB,
    PPELayerType_MULT,
    PPELayerType_LRELU,
    PPELayerType_LRELUX,
    PPELayerType_LPRELU,
    PPELayerType_MAXIMUM,
    PPELayerType_MINIMUM,
    PPELayerType_CEIL,
    PPELayerType_FLOOR,
    PPELayerType_AND,
    PPELayerType_OR,
    PPELayerType_XOR,
    PPELayerType_NOT,
    PPELayerType_ABS,
    PPELayerType_NEG,
    PPELayerType_POW,
    PPELayerType_EXP,
    PPELayerType_SIGMOID,
    PPELayerType_TANH,
    PPELayerType_SQRT,
    PPELayerType_RSQRT,
    PPELayerType_FLEXARB
  };
  return values;
}

inline const char * const *EnumNamesPPELayerType() {
  static const char * const names[] = {
    "STORE",
    "LOAD",
    "CLEAR",
    "NOOP",
    "HALT",
    "ADD",
    "SUB",
    "MULT",
    "LRELU",
    "LRELUX",
    "LPRELU",
    "MAXIMUM",
    "MINIMUM",
    "CEIL",
    "FLOOR",
    "AND",
    "OR",
    "XOR",
    "NOT",
    "ABS",
    "NEG",
    "POW",
    "EXP",
    "SIGMOID",
    "TANH",
    "SQRT",
    "RSQRT",
    "FLEXARB",
    nullptr
  };
  return names;
}

inline const char *EnumNamePPELayerType(PPELayerType e) {
  if (e < PPELayerType_STORE || e > PPELayerType_FLEXARB) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesPPELayerType()[index];
}

enum MPE_Mode {
  MPE_Mode_VECTOR = 0,
  MPE_Mode_MATRIX = 1,
  MPE_Mode_MIN = MPE_Mode_VECTOR,
  MPE_Mode_MAX = MPE_Mode_MATRIX
};

inline const MPE_Mode (&EnumValuesMPE_Mode())[2] {
  static const MPE_Mode values[] = {
    MPE_Mode_VECTOR,
    MPE_Mode_MATRIX
  };
  return values;
}

inline const char * const *EnumNamesMPE_Mode() {
  static const char * const names[] = {
    "VECTOR",
    "MATRIX",
    nullptr
  };
  return names;
}

inline const char *EnumNameMPE_Mode(MPE_Mode e) {
  if (e < MPE_Mode_VECTOR || e > MPE_Mode_MATRIX) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesMPE_Mode()[index];
}

struct PPEGenericTask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OPS = 4,
    VT_CLAMPVALUE = 6,
    VT_RELUNEGSLOPE = 8,
    VT_RELUPOSSLOPE = 10,
    VT_SCALE_DATA = 12
  };
  const flatbuffers::Vector<int16_t> *Ops() const {
    return GetPointer<const flatbuffers::Vector<int16_t> *>(VT_OPS);
  }
  uint32_t ClampValue() const {
    return GetField<uint32_t>(VT_CLAMPVALUE, 0);
  }
  uint32_t ReLuNegSlope() const {
    return GetField<uint32_t>(VT_RELUNEGSLOPE, 0);
  }
  uint32_t ReLuPosSlope() const {
    return GetField<uint32_t>(VT_RELUPOSSLOPE, 0);
  }
  const TensorReference *scale_data() const {
    return GetPointer<const TensorReference *>(VT_SCALE_DATA);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_OPS) &&
           verifier.VerifyVector(Ops()) &&
           VerifyField<uint32_t>(verifier, VT_CLAMPVALUE) &&
           VerifyField<uint32_t>(verifier, VT_RELUNEGSLOPE) &&
           VerifyField<uint32_t>(verifier, VT_RELUPOSSLOPE) &&
           VerifyOffset(verifier, VT_SCALE_DATA) &&
           verifier.VerifyTable(scale_data()) &&
           verifier.EndTable();
  }
};

struct PPEGenericTaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_Ops(flatbuffers::Offset<flatbuffers::Vector<int16_t>> Ops) {
    fbb_.AddOffset(PPEGenericTask::VT_OPS, Ops);
  }
  void add_ClampValue(uint32_t ClampValue) {
    fbb_.AddElement<uint32_t>(PPEGenericTask::VT_CLAMPVALUE, ClampValue, 0);
  }
  void add_ReLuNegSlope(uint32_t ReLuNegSlope) {
    fbb_.AddElement<uint32_t>(PPEGenericTask::VT_RELUNEGSLOPE, ReLuNegSlope, 0);
  }
  void add_ReLuPosSlope(uint32_t ReLuPosSlope) {
    fbb_.AddElement<uint32_t>(PPEGenericTask::VT_RELUPOSSLOPE, ReLuPosSlope, 0);
  }
  void add_scale_data(flatbuffers::Offset<TensorReference> scale_data) {
    fbb_.AddOffset(PPEGenericTask::VT_SCALE_DATA, scale_data);
  }
  explicit PPEGenericTaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  PPEGenericTaskBuilder &operator=(const PPEGenericTaskBuilder &);
  flatbuffers::Offset<PPEGenericTask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<PPEGenericTask>(end);
    return o;
  }
};

inline flatbuffers::Offset<PPEGenericTask> CreatePPEGenericTask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<int16_t>> Ops = 0,
    uint32_t ClampValue = 0,
    uint32_t ReLuNegSlope = 0,
    uint32_t ReLuPosSlope = 0,
    flatbuffers::Offset<TensorReference> scale_data = 0) {
  PPEGenericTaskBuilder builder_(_fbb);
  builder_.add_scale_data(scale_data);
  builder_.add_ReLuPosSlope(ReLuPosSlope);
  builder_.add_ReLuNegSlope(ReLuNegSlope);
  builder_.add_ClampValue(ClampValue);
  builder_.add_Ops(Ops);
  return builder_.Finish();
}

inline flatbuffers::Offset<PPEGenericTask> CreatePPEGenericTaskDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int16_t> *Ops = nullptr,
    uint32_t ClampValue = 0,
    uint32_t ReLuNegSlope = 0,
    uint32_t ReLuPosSlope = 0,
    flatbuffers::Offset<TensorReference> scale_data = 0) {
  auto Ops__ = Ops ? _fbb.CreateVector<int16_t>(*Ops) : 0;
  return MVCNN::CreatePPEGenericTask(
      _fbb,
      Ops__,
      ClampValue,
      ReLuNegSlope,
      ReLuPosSlope,
      scale_data);
}

struct DPUInvariantFields FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OP = 4,
    VT_PPE_TASK = 6,
    VT_CLUSTERID = 8,
    VT_KERNELH = 10,
    VT_KERNELW = 12,
    VT_KERNEL_STRIDEH = 14,
    VT_KERNEL_STRIDEW = 16,
    VT_PADLEFT = 18,
    VT_PADRIGHT = 20,
    VT_PADTOP = 22,
    VT_PADBOTTOM = 24,
    VT_INPUT_DATA = 26,
    VT_OUTPUT_DATA = 28,
    VT_WEIGHTS_DATA = 30,
    VT_BIAS_DATA = 32,
    VT_MPE_MODE = 34
  };
  DPULayerType op() const {
    return static_cast<DPULayerType>(GetField<int8_t>(VT_OP, 0));
  }
  const PPEGenericTask *ppe_task() const {
    return GetPointer<const PPEGenericTask *>(VT_PPE_TASK);
  }
  int32_t clusterID() const {
    return GetField<int32_t>(VT_CLUSTERID, 0);
  }
  int16_t kernelH() const {
    return GetField<int16_t>(VT_KERNELH, 0);
  }
  int16_t kernelW() const {
    return GetField<int16_t>(VT_KERNELW, 0);
  }
  int16_t kernel_strideH() const {
    return GetField<int16_t>(VT_KERNEL_STRIDEH, 0);
  }
  int16_t kernel_strideW() const {
    return GetField<int16_t>(VT_KERNEL_STRIDEW, 0);
  }
  int16_t padLeft() const {
    return GetField<int16_t>(VT_PADLEFT, 0);
  }
  int16_t padRight() const {
    return GetField<int16_t>(VT_PADRIGHT, 0);
  }
  int16_t padTop() const {
    return GetField<int16_t>(VT_PADTOP, 0);
  }
  int16_t padBottom() const {
    return GetField<int16_t>(VT_PADBOTTOM, 0);
  }
  const TensorReference *input_data() const {
    return GetPointer<const TensorReference *>(VT_INPUT_DATA);
  }
  const TensorReference *output_data() const {
    return GetPointer<const TensorReference *>(VT_OUTPUT_DATA);
  }
  const TensorReference *weights_data() const {
    return GetPointer<const TensorReference *>(VT_WEIGHTS_DATA);
  }
  const TensorReference *bias_data() const {
    return GetPointer<const TensorReference *>(VT_BIAS_DATA);
  }
  MPE_Mode mpe_mode() const {
    return static_cast<MPE_Mode>(GetField<int8_t>(VT_MPE_MODE, 0));
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int8_t>(verifier, VT_OP) &&
           VerifyOffset(verifier, VT_PPE_TASK) &&
           verifier.VerifyTable(ppe_task()) &&
           VerifyField<int32_t>(verifier, VT_CLUSTERID) &&
           VerifyField<int16_t>(verifier, VT_KERNELH) &&
           VerifyField<int16_t>(verifier, VT_KERNELW) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_STRIDEH) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_STRIDEW) &&
           VerifyField<int16_t>(verifier, VT_PADLEFT) &&
           VerifyField<int16_t>(verifier, VT_PADRIGHT) &&
           VerifyField<int16_t>(verifier, VT_PADTOP) &&
           VerifyField<int16_t>(verifier, VT_PADBOTTOM) &&
           VerifyOffset(verifier, VT_INPUT_DATA) &&
           verifier.VerifyTable(input_data()) &&
           VerifyOffset(verifier, VT_OUTPUT_DATA) &&
           verifier.VerifyTable(output_data()) &&
           VerifyOffset(verifier, VT_WEIGHTS_DATA) &&
           verifier.VerifyTable(weights_data()) &&
           VerifyOffset(verifier, VT_BIAS_DATA) &&
           verifier.VerifyTable(bias_data()) &&
           VerifyField<int8_t>(verifier, VT_MPE_MODE) &&
           verifier.EndTable();
  }
};

struct DPUInvariantFieldsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_op(DPULayerType op) {
    fbb_.AddElement<int8_t>(DPUInvariantFields::VT_OP, static_cast<int8_t>(op), 0);
  }
  void add_ppe_task(flatbuffers::Offset<PPEGenericTask> ppe_task) {
    fbb_.AddOffset(DPUInvariantFields::VT_PPE_TASK, ppe_task);
  }
  void add_clusterID(int32_t clusterID) {
    fbb_.AddElement<int32_t>(DPUInvariantFields::VT_CLUSTERID, clusterID, 0);
  }
  void add_kernelH(int16_t kernelH) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_KERNELH, kernelH, 0);
  }
  void add_kernelW(int16_t kernelW) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_KERNELW, kernelW, 0);
  }
  void add_kernel_strideH(int16_t kernel_strideH) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_KERNEL_STRIDEH, kernel_strideH, 0);
  }
  void add_kernel_strideW(int16_t kernel_strideW) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_KERNEL_STRIDEW, kernel_strideW, 0);
  }
  void add_padLeft(int16_t padLeft) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_PADLEFT, padLeft, 0);
  }
  void add_padRight(int16_t padRight) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_PADRIGHT, padRight, 0);
  }
  void add_padTop(int16_t padTop) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_PADTOP, padTop, 0);
  }
  void add_padBottom(int16_t padBottom) {
    fbb_.AddElement<int16_t>(DPUInvariantFields::VT_PADBOTTOM, padBottom, 0);
  }
  void add_input_data(flatbuffers::Offset<TensorReference> input_data) {
    fbb_.AddOffset(DPUInvariantFields::VT_INPUT_DATA, input_data);
  }
  void add_output_data(flatbuffers::Offset<TensorReference> output_data) {
    fbb_.AddOffset(DPUInvariantFields::VT_OUTPUT_DATA, output_data);
  }
  void add_weights_data(flatbuffers::Offset<TensorReference> weights_data) {
    fbb_.AddOffset(DPUInvariantFields::VT_WEIGHTS_DATA, weights_data);
  }
  void add_bias_data(flatbuffers::Offset<TensorReference> bias_data) {
    fbb_.AddOffset(DPUInvariantFields::VT_BIAS_DATA, bias_data);
  }
  void add_mpe_mode(MPE_Mode mpe_mode) {
    fbb_.AddElement<int8_t>(DPUInvariantFields::VT_MPE_MODE, static_cast<int8_t>(mpe_mode), 0);
  }
  explicit DPUInvariantFieldsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  DPUInvariantFieldsBuilder &operator=(const DPUInvariantFieldsBuilder &);
  flatbuffers::Offset<DPUInvariantFields> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<DPUInvariantFields>(end);
    return o;
  }
};

inline flatbuffers::Offset<DPUInvariantFields> CreateDPUInvariantFields(
    flatbuffers::FlatBufferBuilder &_fbb,
    DPULayerType op = DPULayerType_CONV,
    flatbuffers::Offset<PPEGenericTask> ppe_task = 0,
    int32_t clusterID = 0,
    int16_t kernelH = 0,
    int16_t kernelW = 0,
    int16_t kernel_strideH = 0,
    int16_t kernel_strideW = 0,
    int16_t padLeft = 0,
    int16_t padRight = 0,
    int16_t padTop = 0,
    int16_t padBottom = 0,
    flatbuffers::Offset<TensorReference> input_data = 0,
    flatbuffers::Offset<TensorReference> output_data = 0,
    flatbuffers::Offset<TensorReference> weights_data = 0,
    flatbuffers::Offset<TensorReference> bias_data = 0,
    MPE_Mode mpe_mode = MPE_Mode_VECTOR) {
  DPUInvariantFieldsBuilder builder_(_fbb);
  builder_.add_bias_data(bias_data);
  builder_.add_weights_data(weights_data);
  builder_.add_output_data(output_data);
  builder_.add_input_data(input_data);
  builder_.add_clusterID(clusterID);
  builder_.add_ppe_task(ppe_task);
  builder_.add_padBottom(padBottom);
  builder_.add_padTop(padTop);
  builder_.add_padRight(padRight);
  builder_.add_padLeft(padLeft);
  builder_.add_kernel_strideW(kernel_strideW);
  builder_.add_kernel_strideH(kernel_strideH);
  builder_.add_kernelW(kernelW);
  builder_.add_kernelH(kernelH);
  builder_.add_mpe_mode(mpe_mode);
  builder_.add_op(op);
  return builder_.Finish();
}

struct DPUVariantFields FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_WORKLOADID = 4,
    VT_OUTPUT_X_INDEX = 6,
    VT_OUTPUT_Y_INDEX = 8,
    VT_OUTPUT_Z_INDEX = 10
  };
  int8_t workloadID() const {
    return GetField<int8_t>(VT_WORKLOADID, 0);
  }
  int16_t output_X_index() const {
    return GetField<int16_t>(VT_OUTPUT_X_INDEX, 0);
  }
  int16_t output_Y_index() const {
    return GetField<int16_t>(VT_OUTPUT_Y_INDEX, 0);
  }
  int16_t output_Z_index() const {
    return GetField<int16_t>(VT_OUTPUT_Z_INDEX, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int8_t>(verifier, VT_WORKLOADID) &&
           VerifyField<int16_t>(verifier, VT_OUTPUT_X_INDEX) &&
           VerifyField<int16_t>(verifier, VT_OUTPUT_Y_INDEX) &&
           VerifyField<int16_t>(verifier, VT_OUTPUT_Z_INDEX) &&
           verifier.EndTable();
  }
};

struct DPUVariantFieldsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_workloadID(int8_t workloadID) {
    fbb_.AddElement<int8_t>(DPUVariantFields::VT_WORKLOADID, workloadID, 0);
  }
  void add_output_X_index(int16_t output_X_index) {
    fbb_.AddElement<int16_t>(DPUVariantFields::VT_OUTPUT_X_INDEX, output_X_index, 0);
  }
  void add_output_Y_index(int16_t output_Y_index) {
    fbb_.AddElement<int16_t>(DPUVariantFields::VT_OUTPUT_Y_INDEX, output_Y_index, 0);
  }
  void add_output_Z_index(int16_t output_Z_index) {
    fbb_.AddElement<int16_t>(DPUVariantFields::VT_OUTPUT_Z_INDEX, output_Z_index, 0);
  }
  explicit DPUVariantFieldsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  DPUVariantFieldsBuilder &operator=(const DPUVariantFieldsBuilder &);
  flatbuffers::Offset<DPUVariantFields> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<DPUVariantFields>(end);
    return o;
  }
};

inline flatbuffers::Offset<DPUVariantFields> CreateDPUVariantFields(
    flatbuffers::FlatBufferBuilder &_fbb,
    int8_t workloadID = 0,
    int16_t output_X_index = 0,
    int16_t output_Y_index = 0,
    int16_t output_Z_index = 0) {
  DPUVariantFieldsBuilder builder_(_fbb);
  builder_.add_output_Z_index(output_Z_index);
  builder_.add_output_Y_index(output_Y_index);
  builder_.add_output_X_index(output_X_index);
  builder_.add_workloadID(workloadID);
  return builder_.Finish();
}

struct DPUTask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INVARIANT = 4,
    VT_VARIANT = 6
  };
  const DPUInvariantFields *invariant() const {
    return GetPointer<const DPUInvariantFields *>(VT_INVARIANT);
  }
  const flatbuffers::Vector<flatbuffers::Offset<DPUVariantFields>> *variant() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<DPUVariantFields>> *>(VT_VARIANT);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INVARIANT) &&
           verifier.VerifyTable(invariant()) &&
           VerifyOffset(verifier, VT_VARIANT) &&
           verifier.VerifyVector(variant()) &&
           verifier.VerifyVectorOfTables(variant()) &&
           verifier.EndTable();
  }
};

struct DPUTaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_invariant(flatbuffers::Offset<DPUInvariantFields> invariant) {
    fbb_.AddOffset(DPUTask::VT_INVARIANT, invariant);
  }
  void add_variant(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<DPUVariantFields>>> variant) {
    fbb_.AddOffset(DPUTask::VT_VARIANT, variant);
  }
  explicit DPUTaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  DPUTaskBuilder &operator=(const DPUTaskBuilder &);
  flatbuffers::Offset<DPUTask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<DPUTask>(end);
    return o;
  }
};

inline flatbuffers::Offset<DPUTask> CreateDPUTask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<DPUInvariantFields> invariant = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<DPUVariantFields>>> variant = 0) {
  DPUTaskBuilder builder_(_fbb);
  builder_.add_variant(variant);
  builder_.add_invariant(invariant);
  return builder_.Finish();
}

inline flatbuffers::Offset<DPUTask> CreateDPUTaskDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<DPUInvariantFields> invariant = 0,
    const std::vector<flatbuffers::Offset<DPUVariantFields>> *variant = nullptr) {
  auto variant__ = variant ? _fbb.CreateVector<flatbuffers::Offset<DPUVariantFields>>(*variant) : 0;
  return MVCNN::CreateDPUTask(
      _fbb,
      invariant,
      variant__);
}

}  // namespace MVCNN

#endif  // FLATBUFFERS_GENERATED_NNDPU_MVCNN_H_
