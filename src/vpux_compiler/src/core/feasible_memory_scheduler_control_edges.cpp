//
// Copyright (C) 2022 Intel Corporation.
// SPDX-License-Identifier: Apache 2.0
//

#include "vpux/compiler/core/feasible_memory_scheduler_control_edges.hpp"

#include "vpux/compiler/utils/analysis.hpp"
#include "vpux/compiler/utils/rewriter.hpp"

#include "vpux/compiler/core/feasible_scheduler_utils.hpp"
#include "vpux/compiler/dialect/VPUIP/dialect.hpp"

using namespace vpux;

//
// Feasible Memory Scheduler Control Edges support
//

FeasibleMemorySchedulerControlEdges::FeasibleMemorySchedulerControlEdges(
        VPU::MemoryKind memKind, AsyncDepsInfo& depsInfo, AliasesInfo& aliasInfo, Logger log,
        LinearScan<mlir::Value, LinearScanHandler>& scan)
        : _log(log), _memKind(memKind), _depsInfo(depsInfo), _aliasInfo(aliasInfo), _scan(scan) {
    _log.setName("feasible-memory-scheduler-control-edges");
}

// This method will update all AsyncExecOp token dependencies so that resulting
// execution is aligned with order generated by list-scheduler
void FeasibleMemorySchedulerControlEdges::insertDependenciesBasic(
        ArrayRef<FeasibleMemoryScheduler::ScheduledOpInfo> scheduledOps) {
    // Go through all the tasks and add token dependencies between
    // all tasks with start time t to all tasks with time t+1
    _log.trace("Get dependencies based on scheduler time decisions");
    _log = _log.nest();
    for (auto opIt = scheduledOps.begin(); opIt != scheduledOps.end(); opIt++) {
        if (!opIt->isOriginalOp()) {
            continue;
        }

        size_t nextTimeDiff = 0;
        for (auto nextTimeOpIt = opIt; nextTimeOpIt != scheduledOps.end(); nextTimeOpIt++) {
            if (!nextTimeOpIt->isOriginalOp()) {
                continue;
            } else if (nextTimeDiff == 0 && nextTimeOpIt->cycleBegin_ > opIt->cycleBegin_) {
                nextTimeDiff = nextTimeOpIt->cycleBegin_ - opIt->cycleBegin_;
            }

            if (nextTimeDiff != 0) {
                if (nextTimeOpIt->cycleBegin_ == opIt->cycleBegin_ + nextTimeDiff) {
                    // Insert dependency between op at time t to op at
                    // time t+1
                    auto srcAsyncOp = _depsInfo.getExecuteOpAtIndex(opIt->op_);
                    auto dstAsyncOp = _depsInfo.getExecuteOpAtIndex(nextTimeOpIt->op_);
                    _log.trace("Dep: {0} -> {1}", opIt->op_, nextTimeOpIt->op_);
                    VPUX_THROW_UNLESS((srcAsyncOp != nullptr) && (dstAsyncOp != nullptr),
                                      "srcAsyncOp/dstAsyncOp not located based on index");
                    _depsInfo.addDependency(srcAsyncOp, dstAsyncOp);
                } else if (nextTimeOpIt->cycleBegin_ > (opIt->cycleBegin_ + nextTimeDiff)) {
                    break;
                }
            }
        }
    }
    _log = _log.unnest();
}

// This method will update all AsyncExecOp token dependencies for a given executor type
// so that resulting execution is aligned with order generated by list-scheduler
void FeasibleMemorySchedulerControlEdges::insertScheduleOrderDepsForExecutor(
        ArrayRef<FeasibleMemoryScheduler::ScheduledOpInfo> scheduledOps, VPU::ExecutorKind executorKind,
        uint32_t executorInstance) {
    // Go through all the tasks with given executor type
    _log.trace("Insert control edges aligned with schedule order for provided executor");
    _log = _log.nest();
    for (auto opIt = scheduledOps.begin(); opIt != scheduledOps.end(); opIt++) {
        if (!opIt->isOriginalOp()) {
            continue;
        }

        if (opIt->executor != executorKind) {
            continue;
        }

        if (executorInstance >= opIt->executorInstanceMask.size() || !opIt->executorInstanceMask[executorInstance]) {
            continue;
        }

        auto srcAsyncOp = _depsInfo.getExecuteOpAtIndex(opIt->op_);

        size_t nextTimeDiff = 0;
        for (auto nextTimeOpIt = opIt; nextTimeOpIt != scheduledOps.end(); nextTimeOpIt++) {
            if (!nextTimeOpIt->isOriginalOp()) {
                continue;
            }

            if (nextTimeOpIt->executor != executorKind) {
                continue;
            }

            if (executorInstance >= nextTimeOpIt->executorInstanceMask.size() ||
                !nextTimeOpIt->executorInstanceMask[executorInstance]) {
                continue;
            }

            if (nextTimeDiff == 0 && nextTimeOpIt->cycleBegin_ > opIt->cycleBegin_) {
                nextTimeDiff = nextTimeOpIt->cycleBegin_ - opIt->cycleBegin_;
            }

            if (nextTimeDiff != 0) {
                if (nextTimeOpIt->cycleBegin_ == opIt->cycleBegin_ + nextTimeDiff) {
                    // Insert dependency between op at time t to op at
                    // time t+1
                    auto dstAsyncOp = _depsInfo.getExecuteOpAtIndex(nextTimeOpIt->op_);
                    _log.trace("Dep: {0} -> {1}", opIt->op_, nextTimeOpIt->op_);
                    VPUX_THROW_UNLESS((srcAsyncOp != nullptr) && (dstAsyncOp != nullptr),
                                      "srcAsyncOp/dstAsyncOp not located based on index");
                    auto srcCycleEnd = getAsyncExecuteCycleEnd(srcAsyncOp);
                    auto dstCycleBegin = getAsyncExecuteCycleBegin(dstAsyncOp);
                    VPUX_THROW_UNLESS(srcCycleEnd <= dstCycleBegin,
                                      "Order of execution not preserved from {0} with cycleEnd = {1} to {2} with "
                                      "cycleBegin = {3}",
                                      opIt->op_, srcCycleEnd, nextTimeOpIt->op_, dstCycleBegin);
                    _depsInfo.addDependency(srcAsyncOp, dstAsyncOp);
                } else if (nextTimeOpIt->cycleBegin_ > (opIt->cycleBegin_ + nextTimeDiff)) {
                    break;
                }
            }
        }
    }
    _log = _log.unnest();
}

// Insert control flow for overlapping memory regions
void FeasibleMemorySchedulerControlEdges::insertMemoryControlEdges(
        ArrayRef<FeasibleMemoryScheduler::ScheduledOpInfo> scheduledOps) {
    std::list<ScheduledOpOneResource> scheduledOpsResources;

    _log.trace("Insert control edges for overlapping memory resources");

    // Analyze output from feasible scheduler and prepare list of scheduled
    // operations with their resource and time as needed by control edge
    // generation algorithm
    for (auto& scheduledOp : scheduledOps) {
        VPUX_THROW_UNLESS(scheduledOp.isOriginalOp(), "Invalid operation identified for control edge insertion");

        // buffers used by operation, both inputs and outputs
        mlir::DenseSet<mlir::Value> inputBuffers;
        mlir::DenseSet<mlir::Value> outputBuffers;

        // Get operation buffers for all operands. Go through each layer op and
        // store in a set all root buffers
        auto execOp = _depsInfo.getExecuteOpAtIndex(scheduledOp.op_);
        auto* bodyBlock = &execOp.body().front();
        for (auto& innerOp : bodyBlock->getOperations()) {
            if (!mlir::isa<VPUIP::LayerOpInterface>(innerOp)) {
                continue;
            }

            auto inputs = mlir::dyn_cast<VPUIP::LayerOpInterface>(innerOp).getInputs();
            for (const auto& input : inputs) {
                const auto type = input.getType().dyn_cast<vpux::NDTypeInterface>();
                if (type == nullptr || type.getMemoryKind() != _memKind) {
                    continue;
                }
                auto rootBuffers = _aliasInfo.getRoots(input);
                VPUX_THROW_UNLESS(rootBuffers.size() == 1, "Value '{0}' expected to have only one root. Got {1}", input,
                                  rootBuffers.size());
                auto rootBuffer = *rootBuffers.begin();
                inputBuffers.insert(rootBuffer);
            }

            auto outputs = mlir::dyn_cast<VPUIP::LayerOpInterface>(innerOp).getOutputs();
            for (const auto& output : outputs) {
                const auto type = output.getType().dyn_cast<vpux::NDTypeInterface>();
                if (type == nullptr || type.getMemoryKind() != _memKind) {
                    continue;
                }
                auto rootBuffers = _aliasInfo.getRoots(output);
                VPUX_THROW_UNLESS(rootBuffers.size() == 1, "Value '{0}' expected to have only one root. Got {1}",
                                  output, rootBuffers.size());
                auto rootBuffer = *rootBuffers.begin();
                outputBuffers.insert(rootBuffer);
            }
        }

        // For all identified buffers used by operation create separate entries with information
        // about memory ranges to properly identify range producer and consumers at a given time
        for (auto& buf : inputBuffers) {
            if (!isBufAllocOp(buf.getDefiningOp())) {
                continue;
            }
            auto addressStart = _scan.handler().getAddress(buf);
            auto bufSize = _scan.handler().getSize(buf);
            if (bufSize == 0) {
                continue;
            }
            auto addressEnd = addressStart + bufSize - 1;
            _log.trace("op = '{0}'\t time = '{1}'\t input = [{2} - {3}]", scheduledOp.op_, scheduledOp.cycleBegin_,
                       addressStart, addressEnd);
            scheduledOpsResources.push_back(ScheduledOpOneResource(scheduledOp.op_, addressStart, addressEnd,
                                                                   ScheduledOpOneResource::EResRelation::CONSUMER));
        }
        for (auto& buf : outputBuffers) {
            if (!isBufAllocOp(buf.getDefiningOp())) {
                continue;
            }
            auto addressStart = _scan.handler().getAddress(buf);
            auto bufSize = _scan.handler().getSize(buf);
            if (bufSize == 0) {
                continue;
            }
            auto addressEnd = addressStart + bufSize - 1;
            _log.trace("op = '{0}'\t time = '{1}'\t output = [{2} - {3}]", scheduledOp.op_, scheduledOp.cycleBegin_,
                       addressStart, addressEnd);
            scheduledOpsResources.push_back(ScheduledOpOneResource(scheduledOp.op_, addressStart, addressEnd,
                                                                   ScheduledOpOneResource::EResRelation::PRODUCER));
        }
    }

    ControlEdgeSet controlEdges;
    ControlEdgeGenerator<ScheduledOpOneResource> controlEdgeGenerator;
    // Generate control edges for overlapping memory regions
    controlEdgeGenerator.generateControlEdges(scheduledOpsResources.begin(), scheduledOpsResources.end(), controlEdges);

    _log = _log.nest();

    // Apply dependencies from controlEdges set into depsInfo.
    // Later they should be transfered to token based dependencies between AsyncExecuteOps
    updateControlEdgesInDepsInfo(_depsInfo, controlEdges, _log);

    _log = _log.unnest();
}

// After all new dependencies have been prepared call this function to make actual changes in IR
void FeasibleMemorySchedulerControlEdges::updateDependenciesInIR() {
    _log.trace("Update token dependencies in IR");
    _depsInfo.updateTokenDependencies();
}

void vpux::updateControlEdgesInDepsInfo(AsyncDepsInfo& depsInfo, ControlEdgeSet& controlEdges, Logger& log) {
    // Store information about optimized pairs which can coexist based on scheduler decision
    // and assignment of overlapping cycles
    // Map represents all control edges for a given sink node
    //  key - sink node
    //  vector of values - source nodes
    std::map<size_t, SmallVector<size_t>> optimizedEdges;

    for (auto itr = controlEdges.begin(); itr != controlEdges.end(); ++itr) {
        if (itr->_source == itr->_sink) {
            continue;
        }

        auto sourceOp = depsInfo.getExecuteOpAtIndex(itr->_source);
        auto sinkOp = depsInfo.getExecuteOpAtIndex(itr->_sink);

        auto sourceOpCycleStart = getAsyncExecuteCycleBegin(sourceOp);
        auto sourceOpCycleEnd = getAsyncExecuteCycleEnd(sourceOp);
        auto sinkOpCycleStart = getAsyncExecuteCycleBegin(sinkOp);
        auto sinkOpCycleEnd = getAsyncExecuteCycleEnd(sinkOp);

        // If there is cycle overlap then scheduler assumed operations can coexist and there
        // is no need for memory control edge
        if (sinkOpCycleStart < sourceOpCycleEnd && sinkOpCycleEnd > sourceOpCycleStart) {
            optimizedEdges[itr->_sink].push_back(itr->_source);
            continue;
        }

        log.trace("Dep: {0} -> {1}", itr->_source, itr->_sink);
        depsInfo.addDependency(sourceOp, sinkOp);
    }

    if (optimizedEdges.empty()) {
        return;
    }

    // Traverse again to update dependencies from optimized deps sources
    // to destinations of optimized control flow sinks
    for (auto itr = controlEdges.begin(); itr != controlEdges.end(); ++itr) {
        if (itr->_source == itr->_sink) {
            continue;
        }

        auto thisOpOptimEdgesItr = optimizedEdges.find(itr->_source);

        if (thisOpOptimEdgesItr == optimizedEdges.end()) {
            continue;
        }

        for (auto& src : thisOpOptimEdgesItr->second) {
            auto sourceOp = depsInfo.getExecuteOpAtIndex(src);
            auto sinkOp = depsInfo.getExecuteOpAtIndex(itr->_sink);

            log.trace("Dep: {0} -> {1}", itr->_source, itr->_sink);
            depsInfo.addDependency(sourceOp, sinkOp);
        }
    }
}
