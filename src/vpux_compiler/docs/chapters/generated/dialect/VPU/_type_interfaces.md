<!-- Autogenerated by mlir-tblgen; don't manually edit -->
# TypeInterface definitions
## ClusterTypeInterface (`ClusterTypeInterface`)

Interface for generating cluster-aware information for types.

### Methods:
#### `getPerClusterComputeShapes`

```c++
SmallVector<Shape> getPerClusterComputeShapes();
```
@brief Retrieve the array of compute shapes
@warning An important thing to consider with regards to compute shapes,
         is that modes like SEGMENTED and OVERLAPPED take precedence over
         DUPLICATED and MULTICASTED.
         In an example case of a "SEGMENTED | DUPLICATED" (needed for SplitOverK)
         tensor with shape [1, 64, 4, 4], the compute shape in each cluster is
         [1, 16, 4, 4], which is needed when tiling and generating workloads,
         while the allocated shape is [1, 64, 4, 4] (because of duplicated)
         information which is needed for scheduler and strategy manager,
         in order to estimate memory

NOTE: This method *must* be implemented by the user.

#### `getPerClusterComputeShapeOffsets`

```c++
SmallVector<Shape> getPerClusterComputeShapeOffsets();
```
@brief Retrieve the array of compute shape offsets with regards to the full buffer
@warning An important thing to consider with regards to compute offsets,
         is that modes like SEGMENTED and OVERLAPPED take precedence over
         DUPLICATED and MULTICASTED.

NOTE: This method *must* be implemented by the user.

#### `getLargestCompactShape`

```c++
Shape getLargestCompactShape();
```
@brief Get largest compact compute shape
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getCompactShape`

```c++
Shape getCompactShape(int64_t tileInd);
```
@brief Get the compact compute shape for a specific cluster
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getPerClusterPadding`

```c++
SmallVector<vpux::PadInfo> getPerClusterPadding();
```
@brief Retrieve the array of padding for each cluster
@warning This function is needed for getting padding in OVERLAPPED mode

NOTE: This method *must* be implemented by the user.

#### `getPerClusterStridedShapes`

```c++
SmallVector<StridedShape> getPerClusterStridedShapes();
```
@brief Retrieve the array of strided compute shapes
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getLargestStridedShape`

```c++
StridedShape getLargestStridedShape();
```
@brief Get largest strided compute shape
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getStridedShape`

```c++
StridedShape getStridedShape(int64_t tileInd);
```
@brief Get the strided compute shape for a specific cluster
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

## DistributedTypeInterface (`VPU_DistributedTypeInterface`)

Interface for types that work with distributed components.
It is compatible with types that containg multiple types internally.

### Methods:
#### `containsDistributedTypes`

```c++
bool containsDistributedTypes();
```
Returns true if the components are distributed types
NOTE: This method *must* be implemented by the user.

#### `getDistributedTypes`

```c++
SmallVector<mlir::Type> getDistributedTypes();
```
Returns the distributed components
NOTE: This method *must* be implemented by the user.

