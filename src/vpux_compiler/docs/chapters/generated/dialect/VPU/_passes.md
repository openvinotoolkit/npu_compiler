<!-- Autogenerated by mlir-tblgen; don't manually edit -->
### `-add-sparsity-map-to-sparse-activations`: Update type of result for operations which produce SparseTensor type.
Pass updates output type of operations which produce sparsified output. It adds sparsity_map to output tensor type.
Then it propagates type to all users until sparse data consumer is reached.
### `-adjust-memory-space`: Adjusts the tensor location for VPU-driven operations
The pass adjusts the location of tensors that are used by hardware-driven operations

Currently, it surrounds VPU-driven nodes with Copy operations to specify that all the data
that they consume/produce must reside in CMX
### `-cmx-concat`: Move Concat operations from DDR to NNCMX
This pass will try to check if a Concat operation can fit in NNCMX
with few restrictions and if so move the concat from DDR to NNCMX.
### `-convert-scalar-to-tensor`: Convert a scalar input to tensor
Some operations (e.g. Gather) do not support scalar data. This pass converts scalar operands to tensors with one element.
### `-correct-NCE-workloads`: Correct NCE workloads if they do not fit requirements
The pass adjusts workload size for NCEDepthConvolution, NCEMaxPool, NCEAveragePool and NCEPermuteQuantize,
as well as for NCE operations that produce sparse activations.

NCEDepthConvolutionOp, NCEMaxPoolOp and NCEAveragePoolOp require the number of channels to be 16, 32 or 64.
If the number of channels does not match, workload is split.

NCEPermuteQuantizeOp rotates output dimensions and padding may be used to indicate the expansion over height.
It is necessary to subtract pads from respective workload dimensions and then set zero padding.

NCE operations with sparse outputs must have all variants with the same number of channels and the number
of channels has to be a power of two. Additionally, if the NCE op shares a consumer with another NCE op
(directly or indirectly), the number of channels of their variants must be aligned.
### `-detect-in-place-eltwise`: Convert Eltwise operation to read and write to the same buffer in memory
This pass will check if Eltwise operation has input and output buffers of the same size
in memory and mark such Eltwise eligible for inplace execution.
It will write the result into one of the inputs in memory.
### `-ensure-nce-ops-size-requirements`: Ensure hw operations meet size requirements
This pass ensures that hardware operations meet hardware size requirements:
each operation need to have less than 8192 values per dimension. This is done
by tiling such operations into smaller ones.

Note: currently, operations with input channels greater than 8192 will cause this
pass to fail. I introduced allowLargeInputChannels config, in order to bypass this failure for Model_G,
this will be removed when input channel implementation done.
### `-fuse-clamp`: Fuses VPU.Clamp parameters into previous NCE operation
This pass follows `SetupPPEPass` and fuses VPU.Clamp with already existing PPE task.
1. Search for VPU.NCE -> VPU.Clamp pattern
2. Fetch min and max parameters from VPU.Clamp
3. Set clamp_low and clamp_high according to min, max and existing activation
4. Remove VPU.Clamp from the graph
### `-fuse-sparsity-ops`: Fuse subsequent [De]SparsifyOps with SparseOpInterface ops

#### Options
```
-fuse-sparsify : Flag to choose inputs or output will be handled
```
### `-init-compiler`: Initializes compiler for VPU platforms
This pass attaches VPU related compilation parameters to Module attributes and
initializes **IERT Dialect** run-time resources information.

#### Options
```
-vpu-arch          : VPU architecture to compile for
-compilation-mode  : [Optional] Set compilation mode as `ReferenceSW`, `ReferenceHW` or `DefaultHW`
-num-of-dpu-groups : [Optional] Number of available DPU groups
-num-of-dma-ports  : [Optional] Number of available DMA ports
```
### `-isolated-tiling`: Tile layers in isolation so that all their I/O meet the memory capacity
The pass applies tiling to the layers whose memory requirements exceed the capacity available.

The pass tries to split each single layer in isolation, with no smarter heuristics
such as "allow running in parallel" or "allow continious computation in tiles" or any else.

The pass does not use any cost model to optimize the entire layer's processing time. It just
iteratively increases the number of tiles until the the largest tile's memory requirements  meet
the device capacity, and stops there.
### `-lower-sparsity-ops`: Convert Sparsify/Desparsify ops to Eltwise or GroupSparseBufferOp
Convert left Sparsify/Desparsify operations to actual HW ops. Desparsify converts
to Eltwise with aliased inputs, while Sparsify lowering controled by fake-sparsify option.
If fake-sparsify enabled lowering, then fake sparsity map(all values are 1's) will be generated.
Otherwise lowering in same way as Desparsify.

#### Options
```
-fake-sparsify : Flag to choose method of VPU.Sparsify lowering
```
### `-manual-strategy-utils`: Utils for reading or writing a json strategy
Utility allowing to store and write as JSON the current selected strategy from the two strategy passes
createMultiClusterStrategyAssignmentPass() and createPrefetchTilingPass(). And also to manually
overwrite the strategy.

#### Options
```
-write-strategy-to-json       : Flag to enable writing strategy to file
-write-strategy-file-location : Location/path to write strategy file
-read-strategy-from-json      : Flag to enable reading strategy from file
-read-strategy-file-location  : Location/path to read strategy file
```
### `-manual-tiling`: Tile layers with manual strategy
The pass performs manual tiling on layers specified by the user.
### `-multi-cluster-strategy-assignment`: This pass compute the hardware efficiency of layer that is executed as SOH or SOK and assigns the most optimal strategy
### `-optimize-concate-slice-to-slice-concat`: Optimize concate-slice to slice-concat
This pass optimize concat-slice to slice-concat to reduce data copy.
### `-optimize-sparsify-desparsify-pairs`: Optimize common patterns of subsequent sparsify-desparsify ops to remove redundant conversions

#### Options
```
-sparsity-profile : Flag to choose sparsity profile
```
### `-optimize-sparsity-ops`: Optimize additional sparsity patterns
Some optimizations such duplicated Sparsify ops for Eltwise, first Sparsify
or last Desparsify cant be done during WrapOpsInSparsifyDesparsifyPairs pass
until output sparsity wouldnt be fused

#### Options
```
-sparsity-profile : Flag to choose sparsity profile
```
### `-prefetch-tiling`: Tile layers into smaller tiles to enable prefetch pipeline
The pass performs tiling on layers to enable prefetch pipeline.

The pass tries run tiles in parallel.
The 'prefetch' means that the next tile could be loaded in advance when the current tile is computing.

The pass does not consider cost models,
only tiles layers to make at least two tiles could be loaded in CMX memory at the same time.
### `-recompute-sparsity-ptrs`: Recomputes sparsity pointers
Recomputes the sparsity pointers inside the weights table for sparse weights.
### `-resolve-pwl-post-ops`: Resolve requirements for fused PWL post-ops
Ensures the correct quantization ranges are used for fused PWL activation functions.
### `-setup-ppe`: Sets activation function for VPU37XX PPE based on clamp range
Ensures the correct activation function and clamping is used for PPE.
Namely:
* When ReLU shift value is non-zero, set leaky ReLU.
* Otherwise, set NOOP.
* Deduce clamping via output element type.
### `-sparsify-weights`: Sparsify weights for NCE ops
Convert const parameters for NCE ops to sparse types depending on sparsify strategy.
### `-split-NCE-ops-onto-workloads`: Split VPU NCE operation onto workloads
### `-wrap-ops-in-sparsify-pairs`: Wrap operations in pairs of Sparsify-Desparsify
Wraps operations in pairs of Sparsify-Desparify ops. The sparsity profile
will determine which operations will be wrapped:
- profile S0: add SparsifyOp for each input and Sparsify-Desparsify chain for output
- profile S1: add Sparsify-Desparsify chain both for inputs and output

#### Options
```
-sparsity-profile : Flag to choose sparsity profile
```
### `-wrap-vpu-ops-in-ncecluster-tiling`: This pass wraps vpu operations that should be executed across multiple clusters in NCEClusterTiling operations
This pass builds an IR in order to represent multi-cluster compilation. It performs a number of functions.
1) It creates variations of distributed tensors depending on the multi-cluster strategy of the layer.
2) It creates DMA operations DDR->CMX and wraps the DMAs in NCEClusterTiling.
3) It wraps hardware executable operations in NCEClusterTiling.
