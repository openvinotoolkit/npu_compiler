<!-- Autogenerated by mlir-tblgen; don't manually edit -->
### `-adjust-for-vpu`: Adjust IE Dialect IR for VPU target
This "pipeline" pass includes various adaptation passes to adjust the IR for VPU target.
### `-convert-fc-to-conv`: Convert FullyConnected op to Convolution op
This "atomic" pass replaces all FullyConnected ops with Convolution ops.
### `-convert-paddings-to-floor-mode`: Convert Convolution and Pooling layers paddings to FLOOR rouding mode
This "atomic" pass updates padding attributes for Convolution and Pooling layers.
It switches layer rounding mode to FLOOR and updates paddings to satisfy output shape.
### `-convert-precision-to-fp16`: Convert tensors precision from FP32 to FP16
This "atomic" pass replaces all FP32 tensors with FP16.
It updates both function bodies as well as Function signatures.
### `-convert-shape-to-4d`: Convert tensors shapes to 4D
This "atomic" pass replaces all ND tensor with 4D analogues.
Tensor with rank greater than 4 will be treated as an error.
### `-convert-tile-to-per-axis-tiles`: Convert tile op by multiple axes to multiple PerAxisTile ops
This "atomic" pass replaces all Tile op with PerAxisTile ops.
### `-dequantize-const`: Performs constant folding for `Constant -> quant.dcast` case
### `-low-precision`: Low precision transformations
This "pipeline" pass includes all transformations to support low precisions.
### `-merge-fake-quant`: Merge pair `quant.qcast -> quant.dcast` into single IE.FakeQuantize
### `-quantize-const`: Performs constant folding for `Constant -> quant.qcast` case
### `-split-fake-quant`: Splits IE FakeQuantize operations to `quant.qcast -> quant.dcast` pair
### `-use-user-precision`: Use user precisions for entry point function prototype
This "atomic" pass updates the CNNNetwork entry point function prototype
and use user-provided precisions for its operands and results.
The pass inserts Convert operations from/to topology precisions.
