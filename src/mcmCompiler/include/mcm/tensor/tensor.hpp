#ifndef TENSOR_HPP_
#define TENSOR_HPP_

#include <functional>
#include <memory>
#include <algorithm>
#include <vector>
#include <iterator>
#include "include/mcm/base/element.hpp"
#include "include/mcm/tensor/shape.hpp"
#include "include/mcm/tensor/order/order.hpp"
#include "include/mcm/tensor/data_element.hpp"
#include "include/mcm/tensor/dtype/dtype.hpp"
#include "include/mcm/base/exception/argument_error.hpp"
#include "include/mcm/base/exception/value_error.hpp"
#include "include/mcm/tensor/quantization_params.hpp"
#include "include/mcm/target/kmb/workload_struct.hpp"

namespace mv
{

    class Tensor : public Element
    {
    public:
        class MemoryLocation  : public LogSender {
        //TODO :: find better place for it !!
        public:
            enum Location {
                NNCMX = 0,
                UPACMX =1,
                DDR = 2,
                INPUT = 3,
                OUTPUT = 4,
                BLOB = 5,
                VIRTUAL = 6,
                CSRAM = 7,
                DEFAULT = 8
            };
        private:
            Location location_;
            bool forced_;
            //bool relocatable need this?

        public:
            MemoryLocation(const std::string& location);
            MemoryLocation(const Location location) : location_(location),forced_(false) {}
            MemoryLocation() : location_(DEFAULT),forced_(false) {}

            MemoryLocation(const std::string& location, bool forced);
            MemoryLocation(const Location location, bool forced) : location_(location),forced_(forced) {}

            bool operator==(const Location other) { return (location_ == other); }
            bool operator==(std::string& other);
            bool operator==(const MemoryLocation& other) { return (location_ == other.location_);}

            bool operator!=(const Location other) {return  (location_ != other); }
            bool operator!=(std::string& other);
            bool operator!=(const MemoryLocation& other) { return (location_ != other.location_);}

            void force() { forced_ = true;}
            bool isDefault() { return (location_ == DEFAULT); }
            bool isForced() {return forced_;}

//            void set(std::string &location) { location_ = namingMap[location]; }
//            void set(const Location location) { location_ = location; }
//            void set(const MemoryLocation& location) { location_ = location.location_; };
            bool relocate(Location newPlace)
            {
                if(forced_)
                    return false;
                else
                {
                    location_ = newPlace;
                    return true;
                }
            }

            bool relocate(std::string& newPlace);

            bool relocate(MemoryLocation& newPlace)
            {
                return relocate(newPlace.location_);
            }

            std::string toString() const;

            virtual std::string getLogID() const override
            {
                return "MemoryLocation";
            }
        };

    private:
        Shape shape_;
        Order internalOrder_;

        std::vector<std::shared_ptr<std::vector<DataElement>>>  data_;

        std::size_t blockSize_;
        std::vector<std::vector<DataElement>::iterator> blocks_;

        std::shared_ptr<Tensor> sparsityMap_;
        std::shared_ptr<Tensor> storageElement_;
        std::vector<std::shared_ptr<Tensor>> subTensors_;
        std::vector<int64_t> kernelDataOffsets_;
        size_t noneZeroElements_;

        bool elementWiseChecks_(const Tensor& other);
        void elementWiseDouble_(const Tensor& other, const std::function<double(double, double)>& opFunc);
        void elementWiseInt_(const Tensor& other, const std::function<int64_t(int64_t, int64_t)>& opFunc);


        std::vector<std::size_t> indToSub_(const Shape& s, size_t index) const;
        unsigned subToInd_(const Shape& s, const std::vector<std::size_t>& sub) const;
        void populateSparsityMapTensor_();
        void setSubtensorsOrder_(Order order);

    public:
        std::vector<int64_t> getZeroPointsPerChannel() const;

        Tensor(const std::string& name, const Shape& shape, DType dType, Order order);
        Tensor(const std::string& name, const Shape& shape, DType dType, Order order, const mv::QuantizationParams& quantParams);
        Tensor(const std::string& name, const Shape& shape, DType dType, Order order, const std::vector<double>& data);
        Tensor(const std::string& name, const Shape& shape, DType dType, Order order, const std::vector<double>& data, const mv::QuantizationParams& quantParams);
        Tensor(const std::string& name, const Shape& shape, DType dType, Order order, const std::vector<int64_t>& data);
        Tensor(const std::string& name, const Shape& shape, DType dType, Order order, const std::vector<int64_t>& data, const mv::QuantizationParams& quantParams);
        Tensor(const std::string& name, const Shape& shape, DType dType, Order order, const std::vector<mv::DataElement>& data);
        Tensor(const std::string& name, const Shape& shape, DType dType, Order order, const std::vector<mv::DataElement>& data, const mv::QuantizationParams& quantParams);
        Tensor(const Tensor& other);
        ~Tensor();

        void populate(const std::vector<double>& data);
        void populate(const std::vector<double>& data, Order order);
        void populate(const std::vector<int64_t>& data);
        void populate(const std::vector<int64_t>& data, Order order);
        void populate(const std::vector<mv::DataElement>& data);
        void populate(const std::vector<mv::DataElement>& data, Order order);

        void unpopulate();

        // Returns true if the tensor was not sparse and sparsity was set, false otherwise
        bool setSparse();
        /**
         * @brief Binds the data (values vector) of this tensor (slave) to the given master tensor. After this operation data accessed
         * from this tensor will be actually read/written to the master tensor. Using the leftPadding and rightPadding it is possible
         * to select a fragment of the master tensor. Shape of the calling tensor will be modified according to the shape of master tensor
         * and padding values. Data type and data order will be inherited from the master tensor. Automatically sets populated flag.
         * Current implementation will disallow any further reordering (setOrder()) and broadcasting (broadcast()) of both master and slave.
         * @param other Master tensor, must be populated
         * @param leftPadding Vector of values specifing the padding between the bounderies (left-top) of the master tensor and this tensor per dimenision.
         * @param rightPadding Vector of values specifing the padding between the bounderies (right-bottom) of the master tensor and this tensor per dimenision.
         */
        void bindData(Tensor& other, const std::vector<std::size_t>& leftPadding = {}, const std::vector<std::size_t>& rightPadding = {});
        void broadcast(const Shape& shape);

        std::string subTensorInfo() const;

        inline bool isDoubleType() const {
            return getDType().isDoubleType();
        }

        // All floating point datatypes
        // FP16 is underlyingly stored on int64
        inline bool isFloatingPointType() const {
            return getDType().isDoubleType() || getDType() == mv::DType("Float16");
        }

        std::vector<DataElement> getData();
        const std::vector<int64_t> getDataPacked();
        int getZeroValuesCount();
        const std::vector<int64_t> &getKernelDataOffsets();

        std::vector<double> getDoubleData();
        std::vector<int64_t> getIntData();
        void setDType(DType dType);
        DType getDType() const;
        void setOrder(Order order, bool updateSubtensors = false);
        Order getOrder() const;
        Shape getShape() const;
        const Order& getInternalOrder() const;
        void setShape(const Shape& shape);
        void setAddress(int64_t address);
        mv::QuantizationParams getQuantParams();
        void setQuantParams(const mv::QuantizationParams& quantParams);

        void add(const Tensor& other);
        void add(double val);
        void subtract(const Tensor& other);
        void subtract(double val);
        void multiply(const Tensor& other);
        void multiply(double val);
        void divide(const Tensor& other);
        void divide(double val);
        void sqrt();

        DataElement& at(const std::vector<std::size_t>& sub);
        const DataElement& at(const std::vector<std::size_t>& sub) const;
        DataElement& at(std::size_t idx);
        const DataElement& at(std::size_t idx) const;
        DataElement& operator()(std::size_t idx);
        const DataElement& operator()(std::size_t idx) const;
        DataElement& operator()(const std::vector<std::size_t>& sub);
        const DataElement& operator()(const std::vector<std::size_t>& sub) const;

        inline bool hasSubTensors() const
        {
            bool flag = false;
            if (subTensors_.size() > 0)
                flag = true;
            return flag;
        }

        inline size_t numSubTensors() const
        {
            return subTensors_.size();
        }

        inline bool isQuantized() const
        {
            return hasAttr("quantParams");
        }

        inline bool isPopulated() const
        {
            return get<bool>("populated");
        }

        inline bool isSparse() const
        {
            if (hasAttr("sparse"))
                return get<bool>("sparse");
            return false;
        }

        inline bool isBroadcasted() const
        {
            if (hasAttr("broadcasted"))
                return get<bool>("broadcasted");
            return true; //by default is true
        }

        inline bool isAllocatedPerCluster() const
        {
            // SOK non-sparse weights are also serialised individually
            // so that they can be compressed by the HDE
            return hasAttr("splitStrategy") &&
                get<std::string>("splitStrategy") == "SplitOverK" &&
                isPopulatedTensor();
        }

        inline bool isPopulatedTensor()  const
        {
            return !hasAttr("weightTable") &&
                !hasAttr("sparsityMap") &&
                !hasAttr("solvedSparsity") &&
                !hasAttr("dilatedSubConvSM") &&
                !hasAttr("dilatedSubConvSE");
        }

        inline size_t size() const
        {
            return shape_.totalSize();
        }

        inline size_t dataPackedSize() const
        {
            return noneZeroElements_;
        }

        inline std::vector<std::size_t> indToSub(size_t index) const
        {
            return indToSub_(shape_, index);
        }

        inline unsigned subToInd(const std::vector<std::size_t>& sub) const
        {
            return subToInd_(shape_, sub);
        }
        inline std::size_t getAddress() const
        {
            return get<std::size_t>("address");
        }

        std::shared_ptr<Tensor> getSparsityMap() const;
        std::shared_ptr<Tensor> getStorageElement() const;
        Tensor &getSubTensor(uint8_t cluster);

        Tensor& operator=(const Tensor& other);

        std::string toString() const override;
        virtual std::string getLogID() const override;

        std::vector<float> computeNumericStrides() const;
        std::size_t computeTotalSize(unsigned int alignment = 16, bool base = false,
                                     bool fatherTensorAligned = false, bool graphOptimizer = false, bool dilation = false) const;
        std::size_t getClusterSize(unsigned int alignment = 16, bool base = false) const;
        void splitAcrossClusters(std::vector<Workload>, bool splitOverH, bool multicast);
        void splitPopulatedActivationAcrossClusters(std::vector<Workload>, bool splitOverH, bool multicast);
        void shareAcrossClusters(std::vector<Workload>, unsigned int numClusters, bool clustering = true);
        void cleanSubtensors();
        int computeAppropriatePadding() const;
        std::set<std::string> getFlowNames() const;
    };

}

#endif // TENSOR_HPP_
