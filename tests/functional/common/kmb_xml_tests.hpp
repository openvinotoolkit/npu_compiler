// Copyright (C) 2018-2019 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

std::string full_quant_model = R"V0G0N(
    <?xml version="1.0" ?>
    <net batch="1" name="resnet50-int8-fragment" version="5">
    <layers>
        <layer id="0" name="input" precision="FP32" type="Input">
            <output>
                <port id="0">
                    <dim>1</dim>
                    <dim>64</dim>
                    <dim>64</dim>
                    <dim>56</dim>
                </port>
            </output>
        </layer>
        <layer id="1" name="conv1" precision="FP32" type="Convolution">
            <data auto_pad="same_upper" dilations="1,1" group="1" kernel="3,3" output="64" pads_begin="1,1" pads_end="1,1" strides="1,1"/>
            <input>
                <port id="0">
                    <dim>1</dim>
                    <dim>64</dim>
                    <dim>64</dim>
                    <dim>56</dim>
                </port>
            </input>
            <output>
                <port id="3">
                    <dim>1</dim>
                    <dim>64</dim>
                    <dim>64</dim>
                    <dim>56</dim>
                </port>
            </output>
            <blobs>
                <weights offset="0" size="147456"/>
                <biases offset="147456" size="256"/>
            </blobs>
        </layer>
        <layer id="2" name="conv1_relu" precision="FP32" type="ReLU">
            <input>
                <port id="0">
                    <dim>1</dim>
                    <dim>64</dim>
                    <dim>64</dim>
                    <dim>56</dim>
                </port>
            </input>
            <output>
                <port id="1">
                    <dim>1</dim>
                    <dim>64</dim>
                    <dim>64</dim>
                    <dim>56</dim>
                </port>
            </output>
        </layer>
        <layer id="3" name="conv2" precision="FP32" type="Convolution">
            <data auto_pad="same_upper" dilations="1,1" group="1" kernel="1,1" output="256" pads_begin="0,0" pads_end="0,0" strides="1,1"/>
            <input>
                <port id="0">
                    <dim>1</dim>
                    <dim>64</dim>
                    <dim>64</dim>
                    <dim>56</dim>
                </port>
            </input>
            <output>
                <port id="3">
                    <dim>1</dim>
                    <dim>256</dim>
                    <dim>64</dim>
                    <dim>56</dim>
                </port>
            </output>
            <blobs>
                <weights offset="147712" size="65536"/>
                <biases offset="213248" size="1024"/>
            </blobs>
        </layer>
    </layers>
    <edges>
        <edge from-layer="0" from-port="0" to-layer="1" to-port="0"/>
        <edge from-layer="1" from-port="3" to-layer="2" to-port="0"/>
        <edge from-layer="2" from-port="1" to-layer="3" to-port="0"/>
    </edges>
    <statistics>
        <layer>
            <name>input</name>
            <min>0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0</min>
            <max>10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047, 10.544994354248047</max>
        </layer>
        <layer>
            <name>conv1_relu</name>
            <min>0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0</min>
            <max>12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559, 12.449431419372559</max>
        </layer>
        <layer>
            <name>conv2</name>
            <min>-11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415, -11.959489201564415</min>
            <max>10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074, 10.97035851572074</max>
        </layer>
    </statistics>
    </net>
    )V0G0N";

std::string convolution_only = R"V0G0N(
    <net batch="1" name="CONVOLUTION_TEST" version="2">
        <layers>
            <layer id="0" name="input" precision="FP16" type="Input">
                <output>
                    <port id="0">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </output>
            </layer>
            <layer id="2" name="conv_test1" precision="FP16" type="Convolution">
                <data dilations="1,1" group="1" kernel="7,7" output="64" pads_begin="3,3" pads_end="3,3" strides="2,2"/>
                <input>
                    <port id="0">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </input>
                <output>
                    <port id="1">
                        <dim>1</dim>
                        <dim>64</dim>
                        <dim>112</dim>
                        <dim>112</dim>
                    </port>
                </output>
                <blobs>
                    <weights offset="0" size="18816"/>
                    <biases offset="18816" size="128"/>
                </blobs>
            </layer>
        </layers>
        <edges>
            <edge from-layer="0" from-port="0" to-layer="2" to-port="0"/>
        </edges>
    </net>
        )V0G0N";

std::string conv_after_scale_shift = R"V0G0N(
    <net batch="1" name="CONVOLUTION_TEST" version="2">
        <layers>
            <layer id="0" name="input" precision="FP16" type="Input">
                <output>
                    <port id="0">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </output>
            </layer>
            <layer id="1" name="scale_shift1" precision="FP16" type="ScaleShift">
                <input>
                    <port id="0">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </input>
                <output>
                    <port id="1">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </output>
                <blobs>
                    <weights offset="0" size="6"/>
                    <biases offset="6" size="6"/>
                </blobs>
            </layer>
            <layer id="2" name="conv_test1" precision="FP16" type="Convolution">
                <data dilations="1,1" group="1" kernel="7,7" output="64" pads_begin="3,3" pads_end="3,3" strides="2,2"/>
                <input>
                    <port id="0">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </input>
                <output>
                    <port id="1">
                        <dim>1</dim>
                        <dim>64</dim>
                        <dim>112</dim>
                        <dim>112</dim>
                    </port>
                </output>
                <blobs>
                    <weights offset="12" size="18816"/>
                    <biases offset="18828" size="128"/>
                </blobs>
            </layer>
        </layers>
        <edges>
            <edge from-layer="0" from-port="0" to-layer="1" to-port="0"/>
            <edge from-layer="1" from-port="1" to-layer="2" to-port="0"/>
        </edges>
    </net>
        )V0G0N";


std::string pooling_test2 = R"V0G0N(
    <net batch="1" name="POOLING_TEST" version="2">
        <layers>
            <layer id="0" name="input" precision="FP16" type="Input">
                <output>
                    <port id="0">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </output>
            </layer>
            <layer id="1" name="pooling_test" precision="FP16" type="Pooling">
                <data auto_pad="same_upper" exclude-pad="true" kernel="3,3" pads_begin="0,0" pads_end="1,1" pool-method="max" strides="2,2"/>
                <input>
                    <port id="0">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </input>
                <output>
                    <port id="1">
                        <dim>1</dim>
                        <dim>3</dim>
                        <dim>224</dim>
                        <dim>224</dim>
                    </port>
                </output>
            </layer>
            </layers>
        <edges>
            <edge from-layer="0" from-port="0" to-layer="1" to-port="0"/>
        </edges>
    </net>
        )V0G0N";

std::string relu_test_2 = R"V0G0N(
        <net batch="1" name="RELU_TEST" version="2">
            <layers>
                <layer id="0" name="input" precision="FP16" type="Input">
                    <output>
                        <port id="0">
                            <dim>1</dim>
                            <dim>64</dim>
                            <dim>112</dim>
                            <dim>112</dim>
                        </port>
                    </output>
                </layer>
                <layer id="3" name="relu_test" precision="FP16" type="ReLU">
                    <input>
                        <port id="0">
                            <dim>1</dim>
                            <dim>64</dim>
                            <dim>112</dim>
                            <dim>112</dim>
                        </port>
                    </input>
                    <output>
                        <port id="1">
                            <dim>1</dim>
                            <dim>64</dim>
                            <dim>112</dim>
                            <dim>112</dim>
                        </port>
                    </output>
                </layer>
            </layers>
            <edges>
                <edge from-layer="0" from-port="0" to-layer="3" to-port="0"/>
            </edges>
        </net>
            )V0G0N";
