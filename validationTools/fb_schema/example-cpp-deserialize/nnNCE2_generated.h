// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_NNNCE2_MVCNN_H_
#define FLATBUFFERS_GENERATED_NNNCE2_MVCNN_H_

#include "flatbuffers/flatbuffers.h"

#include "memoryManagement_generated.h"
#include "software_generated.h"

namespace MVCNN {

struct PPEFixedFunction;

struct PPETask;

struct NCEInvariantFields;

struct NCEVariantFields;

struct NCE2Task;

enum DPULayerType {
  DPULayerType_CONV = 0,
  DPULayerType_DWCONV = 1,
  DPULayerType_MAXPOOL = 2,
  DPULayerType_AVEPOOL = 3,
  DPULayerType_FCL = 4,
  DPULayerType_ELTWISE = 5,
  DPULayerType_IDENTITY = 6,
  DPULayerType_MIN = DPULayerType_CONV,
  DPULayerType_MAX = DPULayerType_IDENTITY
};

inline const DPULayerType (&EnumValuesDPULayerType())[7] {
  static const DPULayerType values[] = {
    DPULayerType_CONV,
    DPULayerType_DWCONV,
    DPULayerType_MAXPOOL,
    DPULayerType_AVEPOOL,
    DPULayerType_FCL,
    DPULayerType_ELTWISE,
    DPULayerType_IDENTITY
  };
  return values;
}

inline const char * const *EnumNamesDPULayerType() {
  static const char * const names[] = {
    "CONV",
    "DWCONV",
    "MAXPOOL",
    "AVEPOOL",
    "FCL",
    "ELTWISE",
    "IDENTITY",
    nullptr
  };
  return names;
}

inline const char *EnumNameDPULayerType(DPULayerType e) {
  if (e < DPULayerType_CONV || e > DPULayerType_IDENTITY) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesDPULayerType()[index];
}

enum PPELayerType {
  /// Overview:
  /// The possible operations that the PPE unit can perform after
  /// a DPU operation
  /// Fields based on details from 14_08_NCE_PPE.odt
  /// Excluded Layer: BYPASS - If an array of PPELayers is empty, the device should know to run BYPASS
  ///  --- Low-Level Instructions ---
  /// These instructions are only for advanced usage.
  /// Stores register value to memory
  PPELayerType_STORE = 0  /// Loads a register (2 clock cycles)
,
  PPELayerType_LOAD = 1  /// Clears a register
,
  PPELayerType_CLEAR = 2  /// No Operation - Used for delaying (in cases like LOAD).
,
  PPELayerType_NOOP = 3  /// Stops the PPE.
,
  PPELayerType_HALT = 4  ///  --- Element-wise Operations ---
  /// Sum of 2 operands
,
  PPELayerType_ADD = 5  /// Subtraction of 2 operands
,
  PPELayerType_SUB = 6  /// Multiplication of 2 operands
,
  PPELayerType_MULT = 7  ///  --- Rectification Unit Variants ---
,
  PPELayerType_LRELU = 8,
  PPELayerType_LRELUX = 9,
  PPELayerType_LPRELU = 10  ///  --- Threholding & Limits ---
  /// Maximum of two operands
,
  PPELayerType_MAXIMUM = 11  /// Minimum of two operands
,
  PPELayerType_MINIMUM = 12  /// Ceiling of one operand
,
  PPELayerType_CEIL = 13  /// Floor of one operand
,
  PPELayerType_FLOOR = 14  ///  --- Bitwise Operations ---
  /// Bitwise AND of 2 operations
,
  PPELayerType_AND = 15  /// Bitwise OR of 2 operations
,
  PPELayerType_OR = 16  /// Bitwise XOR of 2 operations
,
  PPELayerType_XOR = 17  /// Bitwise NOT of 1 operations
,
  PPELayerType_NOT = 18  /// Bitwise ABS of 1 operations (Signed Only)
,
  PPELayerType_ABS = 19  /// Bitwise NEG of 1 operations (Signed Only)
,
  PPELayerType_NEG = 20  ///  --- Math Operations (i13 scaling required) ---
  /// X^N
,
  PPELayerType_POW = 21  /// Exp(X)
,
  PPELayerType_EXP = 22  /// Sigmoid(X)
,
  PPELayerType_SIGMOID = 23  /// TanH(X)
,
  PPELayerType_TANH = 24  /// SquareRoot(X)
,
  PPELayerType_SQRT = 25  /// 1/SquareRoot(X)
,
  PPELayerType_RSQRT = 26  /// Programmable Math Function
,
  PPELayerType_FLEXARB = 27,
  PPELayerType_MIN = PPELayerType_STORE,
  PPELayerType_MAX = PPELayerType_FLEXARB
};

inline const PPELayerType (&EnumValuesPPELayerType())[28] {
  static const PPELayerType values[] = {
    PPELayerType_STORE,
    PPELayerType_LOAD,
    PPELayerType_CLEAR,
    PPELayerType_NOOP,
    PPELayerType_HALT,
    PPELayerType_ADD,
    PPELayerType_SUB,
    PPELayerType_MULT,
    PPELayerType_LRELU,
    PPELayerType_LRELUX,
    PPELayerType_LPRELU,
    PPELayerType_MAXIMUM,
    PPELayerType_MINIMUM,
    PPELayerType_CEIL,
    PPELayerType_FLOOR,
    PPELayerType_AND,
    PPELayerType_OR,
    PPELayerType_XOR,
    PPELayerType_NOT,
    PPELayerType_ABS,
    PPELayerType_NEG,
    PPELayerType_POW,
    PPELayerType_EXP,
    PPELayerType_SIGMOID,
    PPELayerType_TANH,
    PPELayerType_SQRT,
    PPELayerType_RSQRT,
    PPELayerType_FLEXARB
  };
  return values;
}

inline const char * const *EnumNamesPPELayerType() {
  static const char * const names[] = {
    "STORE",
    "LOAD",
    "CLEAR",
    "NOOP",
    "HALT",
    "ADD",
    "SUB",
    "MULT",
    "LRELU",
    "LRELUX",
    "LPRELU",
    "MAXIMUM",
    "MINIMUM",
    "CEIL",
    "FLOOR",
    "AND",
    "OR",
    "XOR",
    "NOT",
    "ABS",
    "NEG",
    "POW",
    "EXP",
    "SIGMOID",
    "TANH",
    "SQRT",
    "RSQRT",
    "FLEXARB",
    nullptr
  };
  return names;
}

inline const char *EnumNamePPELayerType(PPELayerType e) {
  if (e < PPELayerType_STORE || e > PPELayerType_FLEXARB) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesPPELayerType()[index];
}

enum MPE_Mode {
  /// A layer can be operated on in two different modes
  /// that dictate the shape of the 'working' tensor for hardware.
  /// Choosing these carefully will result in better utilization of
  /// the hardware. Both in terms of balanced work and overall redundancy
  /// Example:  (As both modes have set 16 channels, this is only looking in 2D
  ///            at the height & width)
  ///
  ///   Original Matrix (3x5x16)
  ///            5
  ///       +-+-+-+-+--
  ///       | | | | | |
  ///       +---------+
  ///      3| | | | | |     (16 Channels)
  ///       +---------+
  ///       | | | | | |
  ///       +-+-+-+-+--
  ///
  ///    Using Mode "MATRIX", there will be one row and 3 columns of
  ///    of redundancy.
  ///       +---------------+
  ///       |A|A|A|A|B|B|B|B|
  ///       +---------------+
  ///       |A|A|A|A|B|B|B|B|
  ///       +---------------+
  ///       |A|A|A|A|B|B|B|B|
  ///       +---------------+
  ///       |A|A|A|A|B|B|B|B|
  ///       +---------------+
  ///
  ///   Using mode "VECTOR", there will be no row redundancy, but there is
  ///   11 columns of redundancy
  ///       +-------------------------------+
  ///       |A|A|A|A|A|A|A|A|A|A|A|A|A|A|A|A|
  ///       +-------------------------------+
  ///       |B|B|B|B|B|B|B|B|B|B|B|B|B|B|B|B|
  ///       +-------------------------------+
  ///       |C|C|C|C|C|C|C|C|C|C|C|C|C|C|C|C|
  ///       +-------------------------------+
  ///
  ///   In this case it is wiser to pick the MATRIX mode as it has
  ///    15/32 = 46% Utilization Matrix Mode
  ///   Versus
  ///    15/48 = 31% Utilization for Vector Mode
  MPE_Mode_VECTOR = 0,
  MPE_Mode_MATRIX = 1,
  MPE_Mode_MIN = MPE_Mode_VECTOR,
  MPE_Mode_MAX = MPE_Mode_MATRIX
};

inline const MPE_Mode (&EnumValuesMPE_Mode())[2] {
  static const MPE_Mode values[] = {
    MPE_Mode_VECTOR,
    MPE_Mode_MATRIX
  };
  return values;
}

inline const char * const *EnumNamesMPE_Mode() {
  static const char * const names[] = {
    "VECTOR",
    "MATRIX",
    nullptr
  };
  return names;
}

inline const char *EnumNameMPE_Mode(MPE_Mode e) {
  if (e < MPE_Mode_VECTOR || e > MPE_Mode_MATRIX) return "";
  const size_t index = static_cast<int>(e);
  return EnumNamesMPE_Mode()[index];
}

struct PPEFixedFunction FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OPS = 4,
    VT_CLAMP_LOW = 6,
    VT_CLAMP_HIGH = 8
  };
  /// This object describes basic PPE tasks that use
  /// the fixed functionality of the hardware directly.
  /// Up to 16 Operations can be performed, any more and
  /// External reconfiguration is needed.
  const flatbuffers::Vector<int16_t> *Ops() const {
    return GetPointer<const flatbuffers::Vector<int16_t> *>(VT_OPS);
  }
  int32_t Clamp_Low() const {
    return GetField<int32_t>(VT_CLAMP_LOW, 0);
  }
  int32_t Clamp_High() const {
    return GetField<int32_t>(VT_CLAMP_HIGH, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_OPS) &&
           verifier.VerifyVector(Ops()) &&
           VerifyField<int32_t>(verifier, VT_CLAMP_LOW) &&
           VerifyField<int32_t>(verifier, VT_CLAMP_HIGH) &&
           verifier.EndTable();
  }
};

struct PPEFixedFunctionBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_Ops(flatbuffers::Offset<flatbuffers::Vector<int16_t>> Ops) {
    fbb_.AddOffset(PPEFixedFunction::VT_OPS, Ops);
  }
  void add_Clamp_Low(int32_t Clamp_Low) {
    fbb_.AddElement<int32_t>(PPEFixedFunction::VT_CLAMP_LOW, Clamp_Low, 0);
  }
  void add_Clamp_High(int32_t Clamp_High) {
    fbb_.AddElement<int32_t>(PPEFixedFunction::VT_CLAMP_HIGH, Clamp_High, 0);
  }
  explicit PPEFixedFunctionBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  PPEFixedFunctionBuilder &operator=(const PPEFixedFunctionBuilder &);
  flatbuffers::Offset<PPEFixedFunction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<PPEFixedFunction>(end);
    return o;
  }
};

inline flatbuffers::Offset<PPEFixedFunction> CreatePPEFixedFunction(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<int16_t>> Ops = 0,
    int32_t Clamp_Low = 0,
    int32_t Clamp_High = 0) {
  PPEFixedFunctionBuilder builder_(_fbb);
  builder_.add_Clamp_High(Clamp_High);
  builder_.add_Clamp_Low(Clamp_Low);
  builder_.add_Ops(Ops);
  return builder_.Finish();
}

inline flatbuffers::Offset<PPEFixedFunction> CreatePPEFixedFunctionDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int16_t> *Ops = nullptr,
    int32_t Clamp_Low = 0,
    int32_t Clamp_High = 0) {
  auto Ops__ = Ops ? _fbb.CreateVector<int16_t>(*Ops) : 0;
  return MVCNN::CreatePPEFixedFunction(
      _fbb,
      Ops__,
      Clamp_Low,
      Clamp_High);
}

struct PPETask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SCALE_DATA = 4,
    VT_FIXED_FUNCTION = 6
  };
  const TensorReference *scale_data() const {
    return GetPointer<const TensorReference *>(VT_SCALE_DATA);
  }
  const flatbuffers::Vector<flatbuffers::Offset<PPEFixedFunction>> *fixed_function() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<PPEFixedFunction>> *>(VT_FIXED_FUNCTION);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SCALE_DATA) &&
           verifier.VerifyTable(scale_data()) &&
           VerifyOffset(verifier, VT_FIXED_FUNCTION) &&
           verifier.VerifyVector(fixed_function()) &&
           verifier.VerifyVectorOfTables(fixed_function()) &&
           verifier.EndTable();
  }
};

struct PPETaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_scale_data(flatbuffers::Offset<TensorReference> scale_data) {
    fbb_.AddOffset(PPETask::VT_SCALE_DATA, scale_data);
  }
  void add_fixed_function(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<PPEFixedFunction>>> fixed_function) {
    fbb_.AddOffset(PPETask::VT_FIXED_FUNCTION, fixed_function);
  }
  explicit PPETaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  PPETaskBuilder &operator=(const PPETaskBuilder &);
  flatbuffers::Offset<PPETask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<PPETask>(end);
    return o;
  }
};

inline flatbuffers::Offset<PPETask> CreatePPETask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> scale_data = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<PPEFixedFunction>>> fixed_function = 0) {
  PPETaskBuilder builder_(_fbb);
  builder_.add_fixed_function(fixed_function);
  builder_.add_scale_data(scale_data);
  return builder_.Finish();
}

inline flatbuffers::Offset<PPETask> CreatePPETaskDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> scale_data = 0,
    const std::vector<flatbuffers::Offset<PPEFixedFunction>> *fixed_function = nullptr) {
  auto fixed_function__ = fixed_function ? _fbb.CreateVector<flatbuffers::Offset<PPEFixedFunction>>(*fixed_function) : 0;
  return MVCNN::CreatePPETask(
      _fbb,
      scale_data,
      fixed_function__);
}

struct NCEInvariantFields FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DPU_TASK_TYPE = 4,
    VT_PPE_TASK = 6,
    VT_NNSHV_TASK = 8,
    VT_KERNELH = 10,
    VT_KERNELW = 12,
    VT_KERNEL_STRIDEH = 14,
    VT_KERNEL_STRIDEW = 16,
    VT_INPUT_DATA = 18,
    VT_OUTPUT_DATA = 20,
    VT_WEIGHTS_DATA = 22,
    VT_BIAS_DATA = 24
  };
  /// This object describes the common information that any subtasks
  /// share. This is generally layer information.
  /// This object is focussed on sharable items for DPUTasks.
  ///
  /// Each NCE2Task consists of:
  /// - a single DPU operation
  /// - one or more PPE Tasks
  ///    Any additional PPE Tasks past the first task must be triggered by
  ///    A NN Shave task or other external mechanism
  /// - zero or more NN Shave Tasks
  ///    These can be triggered by the PPE Tasks.
  ///
  /// Both DPU and PPE have configurations to 'do nothing' if desired
  /// This must still be specified however.
  ///
  DPULayerType dpu_task_type() const {
    return static_cast<DPULayerType>(GetField<int8_t>(VT_DPU_TASK_TYPE, 0));
  }
  const PPETask *ppe_task() const {
    return GetPointer<const PPETask *>(VT_PPE_TASK);
  }
  const flatbuffers::Vector<flatbuffers::Offset<NNTensorTask>> *nnshv_task() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<NNTensorTask>> *>(VT_NNSHV_TASK);
  }
  int16_t kernelH() const {
    return GetField<int16_t>(VT_KERNELH, 0);
  }
  int16_t kernelW() const {
    return GetField<int16_t>(VT_KERNELW, 0);
  }
  int16_t kernel_strideH() const {
    return GetField<int16_t>(VT_KERNEL_STRIDEH, 0);
  }
  int16_t kernel_strideW() const {
    return GetField<int16_t>(VT_KERNEL_STRIDEW, 0);
  }
  const TensorReference *input_data() const {
    return GetPointer<const TensorReference *>(VT_INPUT_DATA);
  }
  const TensorReference *output_data() const {
    return GetPointer<const TensorReference *>(VT_OUTPUT_DATA);
  }
  const TensorReference *weights_data() const {
    return GetPointer<const TensorReference *>(VT_WEIGHTS_DATA);
  }
  const TensorReference *bias_data() const {
    return GetPointer<const TensorReference *>(VT_BIAS_DATA);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int8_t>(verifier, VT_DPU_TASK_TYPE) &&
           VerifyOffset(verifier, VT_PPE_TASK) &&
           verifier.VerifyTable(ppe_task()) &&
           VerifyOffset(verifier, VT_NNSHV_TASK) &&
           verifier.VerifyVector(nnshv_task()) &&
           verifier.VerifyVectorOfTables(nnshv_task()) &&
           VerifyField<int16_t>(verifier, VT_KERNELH) &&
           VerifyField<int16_t>(verifier, VT_KERNELW) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_STRIDEH) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_STRIDEW) &&
           VerifyOffset(verifier, VT_INPUT_DATA) &&
           verifier.VerifyTable(input_data()) &&
           VerifyOffset(verifier, VT_OUTPUT_DATA) &&
           verifier.VerifyTable(output_data()) &&
           VerifyOffset(verifier, VT_WEIGHTS_DATA) &&
           verifier.VerifyTable(weights_data()) &&
           VerifyOffset(verifier, VT_BIAS_DATA) &&
           verifier.VerifyTable(bias_data()) &&
           verifier.EndTable();
  }
};

struct NCEInvariantFieldsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_dpu_task_type(DPULayerType dpu_task_type) {
    fbb_.AddElement<int8_t>(NCEInvariantFields::VT_DPU_TASK_TYPE, static_cast<int8_t>(dpu_task_type), 0);
  }
  void add_ppe_task(flatbuffers::Offset<PPETask> ppe_task) {
    fbb_.AddOffset(NCEInvariantFields::VT_PPE_TASK, ppe_task);
  }
  void add_nnshv_task(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NNTensorTask>>> nnshv_task) {
    fbb_.AddOffset(NCEInvariantFields::VT_NNSHV_TASK, nnshv_task);
  }
  void add_kernelH(int16_t kernelH) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNELH, kernelH, 0);
  }
  void add_kernelW(int16_t kernelW) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNELW, kernelW, 0);
  }
  void add_kernel_strideH(int16_t kernel_strideH) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_STRIDEH, kernel_strideH, 0);
  }
  void add_kernel_strideW(int16_t kernel_strideW) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_STRIDEW, kernel_strideW, 0);
  }
  void add_input_data(flatbuffers::Offset<TensorReference> input_data) {
    fbb_.AddOffset(NCEInvariantFields::VT_INPUT_DATA, input_data);
  }
  void add_output_data(flatbuffers::Offset<TensorReference> output_data) {
    fbb_.AddOffset(NCEInvariantFields::VT_OUTPUT_DATA, output_data);
  }
  void add_weights_data(flatbuffers::Offset<TensorReference> weights_data) {
    fbb_.AddOffset(NCEInvariantFields::VT_WEIGHTS_DATA, weights_data);
  }
  void add_bias_data(flatbuffers::Offset<TensorReference> bias_data) {
    fbb_.AddOffset(NCEInvariantFields::VT_BIAS_DATA, bias_data);
  }
  explicit NCEInvariantFieldsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NCEInvariantFieldsBuilder &operator=(const NCEInvariantFieldsBuilder &);
  flatbuffers::Offset<NCEInvariantFields> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NCEInvariantFields>(end);
    return o;
  }
};

inline flatbuffers::Offset<NCEInvariantFields> CreateNCEInvariantFields(
    flatbuffers::FlatBufferBuilder &_fbb,
    DPULayerType dpu_task_type = DPULayerType_CONV,
    flatbuffers::Offset<PPETask> ppe_task = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NNTensorTask>>> nnshv_task = 0,
    int16_t kernelH = 0,
    int16_t kernelW = 0,
    int16_t kernel_strideH = 0,
    int16_t kernel_strideW = 0,
    flatbuffers::Offset<TensorReference> input_data = 0,
    flatbuffers::Offset<TensorReference> output_data = 0,
    flatbuffers::Offset<TensorReference> weights_data = 0,
    flatbuffers::Offset<TensorReference> bias_data = 0) {
  NCEInvariantFieldsBuilder builder_(_fbb);
  builder_.add_bias_data(bias_data);
  builder_.add_weights_data(weights_data);
  builder_.add_output_data(output_data);
  builder_.add_input_data(input_data);
  builder_.add_nnshv_task(nnshv_task);
  builder_.add_ppe_task(ppe_task);
  builder_.add_kernel_strideW(kernel_strideW);
  builder_.add_kernel_strideH(kernel_strideH);
  builder_.add_kernelW(kernelW);
  builder_.add_kernelH(kernelH);
  builder_.add_dpu_task_type(dpu_task_type);
  return builder_.Finish();
}

inline flatbuffers::Offset<NCEInvariantFields> CreateNCEInvariantFieldsDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    DPULayerType dpu_task_type = DPULayerType_CONV,
    flatbuffers::Offset<PPETask> ppe_task = 0,
    const std::vector<flatbuffers::Offset<NNTensorTask>> *nnshv_task = nullptr,
    int16_t kernelH = 0,
    int16_t kernelW = 0,
    int16_t kernel_strideH = 0,
    int16_t kernel_strideW = 0,
    flatbuffers::Offset<TensorReference> input_data = 0,
    flatbuffers::Offset<TensorReference> output_data = 0,
    flatbuffers::Offset<TensorReference> weights_data = 0,
    flatbuffers::Offset<TensorReference> bias_data = 0) {
  auto nnshv_task__ = nnshv_task ? _fbb.CreateVector<flatbuffers::Offset<NNTensorTask>>(*nnshv_task) : 0;
  return MVCNN::CreateNCEInvariantFields(
      _fbb,
      dpu_task_type,
      ppe_task,
      nnshv_task__,
      kernelH,
      kernelW,
      kernel_strideH,
      kernel_strideW,
      input_data,
      output_data,
      weights_data,
      bias_data);
}

struct NCEVariantFields FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_CLUSTERID = 4,
    VT_WORKLOADID = 6,
    VT_MPE_MODE = 8,
    VT_PADLEFT = 10,
    VT_PADRIGHT = 12,
    VT_PADTOP = 14,
    VT_PADBOTTOM = 16,
    VT_WORKLOAD_START_X = 18,
    VT_WORKLOAD_START_Y = 20,
    VT_WORKLOAD_START_Z = 22,
    VT_WORKLOAD_END_X = 24,
    VT_WORKLOAD_END_Y = 26,
    VT_WORKLOAD_END_Z = 28
  };
  /// This object describes the information that any subtasks
  /// vary on. This is generally resource and position information.
  int32_t clusterID() const {
    return GetField<int32_t>(VT_CLUSTERID, 0);
  }
  int8_t workloadID() const {
    return GetField<int8_t>(VT_WORKLOADID, 0);
  }
  MPE_Mode mpe_mode() const {
    return static_cast<MPE_Mode>(GetField<int8_t>(VT_MPE_MODE, 0));
  }
  int16_t padLeft() const {
    return GetField<int16_t>(VT_PADLEFT, 0);
  }
  int16_t padRight() const {
    return GetField<int16_t>(VT_PADRIGHT, 0);
  }
  int16_t padTop() const {
    return GetField<int16_t>(VT_PADTOP, 0);
  }
  int16_t padBottom() const {
    return GetField<int16_t>(VT_PADBOTTOM, 0);
  }
  /// Co-Ordinates for the starting and ending location of this workload.
  /// Internally, the hardware will increment in steps of MPE_MODE's size
  /// from "start" until it reaches "end".
  int16_t workload_start_X() const {
    return GetField<int16_t>(VT_WORKLOAD_START_X, 0);
  }
  int16_t workload_start_Y() const {
    return GetField<int16_t>(VT_WORKLOAD_START_Y, 0);
  }
  int16_t workload_start_Z() const {
    return GetField<int16_t>(VT_WORKLOAD_START_Z, 0);
  }
  int16_t workload_end_X() const {
    return GetField<int16_t>(VT_WORKLOAD_END_X, 0);
  }
  int16_t workload_end_Y() const {
    return GetField<int16_t>(VT_WORKLOAD_END_Y, 0);
  }
  int16_t workload_end_Z() const {
    return GetField<int16_t>(VT_WORKLOAD_END_Z, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_CLUSTERID) &&
           VerifyField<int8_t>(verifier, VT_WORKLOADID) &&
           VerifyField<int8_t>(verifier, VT_MPE_MODE) &&
           VerifyField<int16_t>(verifier, VT_PADLEFT) &&
           VerifyField<int16_t>(verifier, VT_PADRIGHT) &&
           VerifyField<int16_t>(verifier, VT_PADTOP) &&
           VerifyField<int16_t>(verifier, VT_PADBOTTOM) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_START_X) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_START_Y) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_START_Z) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_END_X) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_END_Y) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_END_Z) &&
           verifier.EndTable();
  }
};

struct NCEVariantFieldsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_clusterID(int32_t clusterID) {
    fbb_.AddElement<int32_t>(NCEVariantFields::VT_CLUSTERID, clusterID, 0);
  }
  void add_workloadID(int8_t workloadID) {
    fbb_.AddElement<int8_t>(NCEVariantFields::VT_WORKLOADID, workloadID, 0);
  }
  void add_mpe_mode(MPE_Mode mpe_mode) {
    fbb_.AddElement<int8_t>(NCEVariantFields::VT_MPE_MODE, static_cast<int8_t>(mpe_mode), 0);
  }
  void add_padLeft(int16_t padLeft) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADLEFT, padLeft, 0);
  }
  void add_padRight(int16_t padRight) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADRIGHT, padRight, 0);
  }
  void add_padTop(int16_t padTop) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADTOP, padTop, 0);
  }
  void add_padBottom(int16_t padBottom) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADBOTTOM, padBottom, 0);
  }
  void add_workload_start_X(int16_t workload_start_X) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_START_X, workload_start_X, 0);
  }
  void add_workload_start_Y(int16_t workload_start_Y) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_START_Y, workload_start_Y, 0);
  }
  void add_workload_start_Z(int16_t workload_start_Z) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_START_Z, workload_start_Z, 0);
  }
  void add_workload_end_X(int16_t workload_end_X) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_END_X, workload_end_X, 0);
  }
  void add_workload_end_Y(int16_t workload_end_Y) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_END_Y, workload_end_Y, 0);
  }
  void add_workload_end_Z(int16_t workload_end_Z) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_END_Z, workload_end_Z, 0);
  }
  explicit NCEVariantFieldsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NCEVariantFieldsBuilder &operator=(const NCEVariantFieldsBuilder &);
  flatbuffers::Offset<NCEVariantFields> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NCEVariantFields>(end);
    return o;
  }
};

inline flatbuffers::Offset<NCEVariantFields> CreateNCEVariantFields(
    flatbuffers::FlatBufferBuilder &_fbb,
    int32_t clusterID = 0,
    int8_t workloadID = 0,
    MPE_Mode mpe_mode = MPE_Mode_VECTOR,
    int16_t padLeft = 0,
    int16_t padRight = 0,
    int16_t padTop = 0,
    int16_t padBottom = 0,
    int16_t workload_start_X = 0,
    int16_t workload_start_Y = 0,
    int16_t workload_start_Z = 0,
    int16_t workload_end_X = 0,
    int16_t workload_end_Y = 0,
    int16_t workload_end_Z = 0) {
  NCEVariantFieldsBuilder builder_(_fbb);
  builder_.add_clusterID(clusterID);
  builder_.add_workload_end_Z(workload_end_Z);
  builder_.add_workload_end_Y(workload_end_Y);
  builder_.add_workload_end_X(workload_end_X);
  builder_.add_workload_start_Z(workload_start_Z);
  builder_.add_workload_start_Y(workload_start_Y);
  builder_.add_workload_start_X(workload_start_X);
  builder_.add_padBottom(padBottom);
  builder_.add_padTop(padTop);
  builder_.add_padRight(padRight);
  builder_.add_padLeft(padLeft);
  builder_.add_mpe_mode(mpe_mode);
  builder_.add_workloadID(workloadID);
  return builder_.Finish();
}

struct NCE2Task FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INVARIANT = 4,
    VT_VARIANT = 6
  };
  /// This object describes an entire network layer that will be operated
  /// on by the NCE's DPUs, PPEs and NNSHV Assist Library.
  ///
  /// The layer is likely to be split into different "workloads" -
  /// subsections of the layer for the processors to split work upon.
  ///
  /// Fields common to these subsections are to be stored in the 'invariant'
  /// part of this object.
  /// All per-section information should be contained in the 'variant'
  /// vector. Where there is one entry per 'workload'. If there are no unique
  /// information per-workload, empty objects should still be placed.
  /// There is a 1-to-1 Relationship between DPUs and "Workloads"
  ///
  /// Below is a typical example of splitting a Convolution
  //// across 5 DPUs (1 Cluster)
  ///                                XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
  ///                         XXXXXXX          XXXXX              XXXXXX
  ///                     XXXXX            XXXXX               XXX     X
  ///                XXXXX             XXXXX                XXXX       X
  ///         XXXXXXXX             XXXXX               XXXXX           X
  ///    XXXXXX                XXXXX             X XX X               XX
  /// XX-------------------+XXX----------------+XX                 XXXXX
  /// |                    |                   |               XXXX    X
  /// |                    |                   |           XXXXX       X
  /// |                    |        C          |      XXXXXX           X
  /// |         A          |                   |  XXXX               XXX
  /// |                    +-------------------+XXX               XXXX X
  /// |                    |                   |               XXXX    X
  /// |                    |                   |            XXXX       X
  /// +--------------------+        D          |        XXXXX          X
  /// |                    |                   |    XXXXX           X XX
  /// |                    |                   | XXX             XXXX
  /// |                    +-------------------XX             XXXX
  /// |         B          |                   |           XXXX
  /// |                    |                   |        XXXX
  /// |                    |        E          |      XXX
  /// |                    |                   |   XX
  /// +--------------------+-------------------+XX
  ///
  /// Splits for workloads are not limited to different dimensions when using a
  /// Single Cluster.
  /// However, splitting across clusters is limited to splitting over height
  /// and splitting over channels.
  const NCEInvariantFields *invariant() const {
    return GetPointer<const NCEInvariantFields *>(VT_INVARIANT);
  }
  const flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>> *variant() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>> *>(VT_VARIANT);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INVARIANT) &&
           verifier.VerifyTable(invariant()) &&
           VerifyOffset(verifier, VT_VARIANT) &&
           verifier.VerifyVector(variant()) &&
           verifier.VerifyVectorOfTables(variant()) &&
           verifier.EndTable();
  }
};

struct NCE2TaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_invariant(flatbuffers::Offset<NCEInvariantFields> invariant) {
    fbb_.AddOffset(NCE2Task::VT_INVARIANT, invariant);
  }
  void add_variant(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>>> variant) {
    fbb_.AddOffset(NCE2Task::VT_VARIANT, variant);
  }
  explicit NCE2TaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NCE2TaskBuilder &operator=(const NCE2TaskBuilder &);
  flatbuffers::Offset<NCE2Task> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NCE2Task>(end);
    return o;
  }
};

inline flatbuffers::Offset<NCE2Task> CreateNCE2Task(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<NCEInvariantFields> invariant = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>>> variant = 0) {
  NCE2TaskBuilder builder_(_fbb);
  builder_.add_variant(variant);
  builder_.add_invariant(invariant);
  return builder_.Finish();
}

inline flatbuffers::Offset<NCE2Task> CreateNCE2TaskDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<NCEInvariantFields> invariant = 0,
    const std::vector<flatbuffers::Offset<NCEVariantFields>> *variant = nullptr) {
  auto variant__ = variant ? _fbb.CreateVector<flatbuffers::Offset<NCEVariantFields>>(*variant) : 0;
  return MVCNN::CreateNCE2Task(
      _fbb,
      invariant,
      variant__);
}

}  // namespace MVCNN

#endif  // FLATBUFFERS_GENERATED_NNNCE2_MVCNN_H_
