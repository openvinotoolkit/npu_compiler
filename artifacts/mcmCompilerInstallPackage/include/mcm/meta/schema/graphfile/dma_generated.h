// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_DMA_MVCNN_H_
#define FLATBUFFERS_GENERATED_DMA_MVCNN_H_

#include "flatbuffers/flatbuffers.h"

#include "memoryManagement_generated.h"

namespace MVCNN {

struct UPADMATask;
struct UPADMATaskT;

struct NNDMATask;
struct NNDMATaskT;

struct UPADMATaskT : public flatbuffers::NativeTable {
  typedef UPADMATask TableType;
  std::unique_ptr<TensorReferenceT> src;
  std::unique_ptr<TensorReferenceT> dst;
  UPADMATaskT() {
  }
};

struct UPADMATask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef UPADMATaskT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SRC = 4,
    VT_DST = 6
  };
  /// A DMA is essentially a memory transfer from one location
  /// to another. 
  /// Length, Striding information and other such required information is
  /// already contained within the TensorReference objects.
  const TensorReference *src() const {
    return GetPointer<const TensorReference *>(VT_SRC);
  }
  const TensorReference *dst() const {
    return GetPointer<const TensorReference *>(VT_DST);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SRC) &&
           verifier.VerifyTable(src()) &&
           VerifyOffset(verifier, VT_DST) &&
           verifier.VerifyTable(dst()) &&
           verifier.EndTable();
  }
  UPADMATaskT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(UPADMATaskT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<UPADMATask> Pack(flatbuffers::FlatBufferBuilder &_fbb, const UPADMATaskT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct UPADMATaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_src(flatbuffers::Offset<TensorReference> src) {
    fbb_.AddOffset(UPADMATask::VT_SRC, src);
  }
  void add_dst(flatbuffers::Offset<TensorReference> dst) {
    fbb_.AddOffset(UPADMATask::VT_DST, dst);
  }
  explicit UPADMATaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  UPADMATaskBuilder &operator=(const UPADMATaskBuilder &);
  flatbuffers::Offset<UPADMATask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<UPADMATask>(end);
    return o;
  }
};

inline flatbuffers::Offset<UPADMATask> CreateUPADMATask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> src = 0,
    flatbuffers::Offset<TensorReference> dst = 0) {
  UPADMATaskBuilder builder_(_fbb);
  builder_.add_dst(dst);
  builder_.add_src(src);
  return builder_.Finish();
}

flatbuffers::Offset<UPADMATask> CreateUPADMATask(flatbuffers::FlatBufferBuilder &_fbb, const UPADMATaskT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NNDMATaskT : public flatbuffers::NativeTable {
  typedef NNDMATask TableType;
  std::unique_ptr<TensorReferenceT> src;
  std::unique_ptr<TensorReferenceT> dst;
  bool compression;
  uint8_t port;
  NNDMATaskT()
      : compression(false),
        port(0) {
  }
};

struct NNDMATask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NNDMATaskT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SRC = 4,
    VT_DST = 6,
    VT_COMPRESSION = 8,
    VT_PORT = 10
  };
  /// The NNDMA has a few additional features.
  /// multiple destination tensors represents copy to several locations
  /// on device it can be DMA bradcast
  const TensorReference *src() const {
    return GetPointer<const TensorReference *>(VT_SRC);
  }
  const TensorReference *dst() const {
    return GetPointer<const TensorReference *>(VT_DST);
  }
  /// The NNDMA also has support for compression while transferring.
  /// This is a simple on/off switch for the feature. 
  bool compression() const {
    return GetField<uint8_t>(VT_COMPRESSION, 0) != 0;
  }
  /// DMA port which the DMA task will execute on
  uint8_t port() const {
    return GetField<uint8_t>(VT_PORT, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SRC) &&
           verifier.VerifyTable(src()) &&
           VerifyOffset(verifier, VT_DST) &&
           verifier.VerifyTable(dst()) &&
           VerifyField<uint8_t>(verifier, VT_COMPRESSION) &&
           VerifyField<uint8_t>(verifier, VT_PORT) &&
           verifier.EndTable();
  }
  NNDMATaskT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NNDMATaskT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<NNDMATask> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NNDMATaskT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NNDMATaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_src(flatbuffers::Offset<TensorReference> src) {
    fbb_.AddOffset(NNDMATask::VT_SRC, src);
  }
  void add_dst(flatbuffers::Offset<TensorReference> dst) {
    fbb_.AddOffset(NNDMATask::VT_DST, dst);
  }
  void add_compression(bool compression) {
    fbb_.AddElement<uint8_t>(NNDMATask::VT_COMPRESSION, static_cast<uint8_t>(compression), 0);
  }
  void add_port(uint8_t port) {
    fbb_.AddElement<uint8_t>(NNDMATask::VT_PORT, port, 0);
  }
  explicit NNDMATaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NNDMATaskBuilder &operator=(const NNDMATaskBuilder &);
  flatbuffers::Offset<NNDMATask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NNDMATask>(end);
    return o;
  }
};

inline flatbuffers::Offset<NNDMATask> CreateNNDMATask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> src = 0,
    flatbuffers::Offset<TensorReference> dst = 0,
    bool compression = false,
    uint8_t port = 0) {
  NNDMATaskBuilder builder_(_fbb);
  builder_.add_dst(dst);
  builder_.add_src(src);
  builder_.add_port(port);
  builder_.add_compression(compression);
  return builder_.Finish();
}

flatbuffers::Offset<NNDMATask> CreateNNDMATask(flatbuffers::FlatBufferBuilder &_fbb, const NNDMATaskT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

inline UPADMATaskT *UPADMATask::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new UPADMATaskT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void UPADMATask::UnPackTo(UPADMATaskT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = src(); if (_e) _o->src = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = dst(); if (_e) _o->dst = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
}

inline flatbuffers::Offset<UPADMATask> UPADMATask::Pack(flatbuffers::FlatBufferBuilder &_fbb, const UPADMATaskT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateUPADMATask(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<UPADMATask> CreateUPADMATask(flatbuffers::FlatBufferBuilder &_fbb, const UPADMATaskT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const UPADMATaskT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _src = _o->src ? CreateTensorReference(_fbb, _o->src.get(), _rehasher) : 0;
  auto _dst = _o->dst ? CreateTensorReference(_fbb, _o->dst.get(), _rehasher) : 0;
  return MVCNN::CreateUPADMATask(
      _fbb,
      _src,
      _dst);
}

inline NNDMATaskT *NNDMATask::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NNDMATaskT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void NNDMATask::UnPackTo(NNDMATaskT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = src(); if (_e) _o->src = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = dst(); if (_e) _o->dst = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = compression(); _o->compression = _e; };
  { auto _e = port(); _o->port = _e; };
}

inline flatbuffers::Offset<NNDMATask> NNDMATask::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NNDMATaskT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNNDMATask(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<NNDMATask> CreateNNDMATask(flatbuffers::FlatBufferBuilder &_fbb, const NNDMATaskT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NNDMATaskT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _src = _o->src ? CreateTensorReference(_fbb, _o->src.get(), _rehasher) : 0;
  auto _dst = _o->dst ? CreateTensorReference(_fbb, _o->dst.get(), _rehasher) : 0;
  auto _compression = _o->compression;
  auto _port = _o->port;
  return MVCNN::CreateNNDMATask(
      _fbb,
      _src,
      _dst,
      _compression,
      _port);
}

}  // namespace MVCNN

#endif  // FLATBUFFERS_GENERATED_DMA_MVCNN_H_
