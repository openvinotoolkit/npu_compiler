// Copyright (C) 2018-2020 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include <fstream>
#include <vector>
#include <memory>
#include <string>
#include <map>
#include <condition_variable>
#include <mutex>
#include <algorithm>

#include <inference_engine.hpp>

#include <format_reader_ptr.h>

#include <samples/common.hpp>
#include <samples/slog.hpp>
#include <samples/args_helper.hpp>
#include <samples/classification_results.h>

#include <sys/stat.h>

#include "test_classification.hpp"

using namespace InferenceEngine;

bool ParseAndCheckCommandLine(int argc, char *argv[]) {
    // ---------------------------Parsing and validation of input args--------------------------------------
    slog::info << "Parsing input parameters" << slog::endl;

    gflags::ParseCommandLineNonHelpFlags(&argc, &argv, true);
    if (FLAGS_h) {
        showUsage();
        showAvailableDevices();
        return false;
    }
    slog::info << "Parsing input parameters" << slog::endl;

    if (FLAGS_i.empty()) {
        throw std::logic_error("Parameter -i is not set");
    }

    if (FLAGS_m.empty()) {
        throw std::logic_error("Parameter -m is not set");
    }

    std::vector<std::string> allowedPrecision = {"u8", "fp16", "fp32"};
    if (!FLAGS_ip.empty()) {
        // input precision is u8, fp16 or fp32 only
        if (std::find(allowedPrecision.begin(), allowedPrecision.end(), FLAGS_ip ) == allowedPrecision.end() )
            throw std::logic_error("Parameter -ip " + FLAGS_ip + " is not supported");
        std::transform(FLAGS_ip.begin(),FLAGS_ip.end(),FLAGS_ip.begin(), ::toupper);
    }

    if (!FLAGS_op.empty()) {
        // output precision is u8, fp16 or fp32 only
        if (std::find(allowedPrecision.begin(), allowedPrecision.end(), FLAGS_op ) == allowedPrecision.end() )
            throw std::logic_error("Parameter -op " + FLAGS_op + " is not supported");
        std::transform(FLAGS_op.begin(),FLAGS_op.end(),FLAGS_op.begin(), ::toupper);
    }

    return true;
}

template <typename T> void writeToFile(std::vector<T>& input, const std::string& dst) {
    std::ofstream dumper(dst, std::ios_base::binary);
    dumper.write(reinterpret_cast<char *>(&input[0]), input.size()*sizeof(T));
    dumper.close();
}

void dumpBlob(const Blob::Ptr& inputBlobPtr, const std::string& dst) {
    std::ofstream dumper(dst, std::ios_base::binary);
    if (dumper.good()) {
        dumper.write(inputBlobPtr->cbuffer().as<char *>(), inputBlobPtr->byteSize());
        // std::cout << "byte size: " << inputBlobPtr->byteSize();
    }
    dumper.close();
}

template <class T_data>
std::vector<T_data> generateSequence(std::size_t dataSize, const std::string& dtype) {
    std::vector<T_data> result(dataSize);
    if (dtype == "U8") {
    for (std::size_t i = 0; i < result.size(); ++i)
        result[i] = (T_data)i;
    } else if (dtype == "FP32") {
        float LO = -10, HI = 10;
        float nummax = RAND_MAX;
        for (std::size_t i = 0; i < result.size(); ++i)
             result[i] = LO + (static_cast <float> (std::rand()) /(nummax/(HI-LO)));
    }
    return result;
}

int main(int argc, char *argv[]) {
    try {
        slog::info << "InferenceEngine: " << GetInferenceEngineVersion() << slog::endl;

        // ------------------------------ Parsing and validation of input args ---------------------------------
        if (!ParseAndCheckCommandLine(argc, argv)) {
            return 0;
        }
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 1. Load inference engine -------------------------------------
        slog::info << "Creating Inference Engine" << slog::endl;

        Core ie;

        if (!FLAGS_l.empty()) {
            // CPU(MKLDNN) extensions are loaded as a shared library and passed as a pointer to base extension
            IExtensionPtr extension_ptr = make_so_pointer<IExtension>(FLAGS_l);
            ie.AddExtension(extension_ptr, "CPU");
            slog::info << "CPU Extension loaded: " << FLAGS_l << slog::endl;
        }
        if (!FLAGS_c.empty()) {
            // clDNN Extensions are loaded from an .xml description and OpenCL kernel files
            ie.SetConfig({{PluginConfigParams::KEY_CONFIG_FILE, FLAGS_c}}, "GPU");
            slog::info << "GPU Extension loaded: " << FLAGS_c << slog::endl;
        }

        /** Printing device version **/
        std::cout << ie.GetVersions(FLAGS_d) << std::endl;
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
        slog::info << "Loading network files" << slog::endl;

        /** Read network model **/
        CNNNetwork network = ie.ReadNetwork(FLAGS_m);
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 3. Configure input & output ---------------------------------------------

        // --------------------------- Prepare input blobs -----------------------------------------------------
        slog::info << "Preparing input blobs" << slog::endl;

        /** Taking information about all topology inputs **/
        InputsDataMap inputInfo(network.getInputsInfo());
        if (inputInfo.size() != 1) throw std::logic_error("Sample supports topologies with 1 input only");

        auto inputInfoItem = *inputInfo.begin();

        // input precision 
        if (!FLAGS_ip.empty()) {
            if (FLAGS_ip == "FP16") inputInfoItem.second->setPrecision(Precision::FP16);
            else if (FLAGS_ip == "FP32") inputInfoItem.second->setPrecision(Precision::FP32);
            else inputInfoItem.second->setPrecision(Precision::U8);
        }
        else 
            inputInfoItem.second->setPrecision(Precision::U8);

        // input layout
        inputInfoItem.second->setLayout(Layout::NCHW);

        Precision inPrecision = inputInfoItem.second->getPrecision();
        SizeVector dims = inputInfoItem.second->getTensorDesc().getDims();

        // validImageNames is needed for classificationresult function. keeping it for now
        std::vector<std::string> validImageNames = {};
        std::vector<std::shared_ptr<unsigned char>> imagesData = {};
        std::vector<uint8_t> inputSeq_u8;
        std::vector<float> inputSeq_fp32;
        bool inputChannelMajor = false;

        size_t totalSize = 1;
        for (auto i : dims)
            totalSize *= i;

        // generate an input or use provided
        if (FLAGS_i.empty()) {
            // auto generate an input
            slog::info << "No image provided, generating a random input..." << slog::endl;

            validImageNames.push_back("Autogenerated");
            inputSeq_u8 = generateSequence<uint8_t>(totalSize, "U8");

        } else {
            if (fileExt(FLAGS_i).compare("dat") == 0 || fileExt(FLAGS_i).compare("bin") == 0)  {
                // use a binary input.dat or .bin
                slog::info << "Using provided binary input..." << slog::endl;
                std::ifstream file(FLAGS_i, std::ios::in | std::ios::binary);
                if (!file.is_open())
                    throw std::logic_error("Input: " + FLAGS_i + " cannot be read!");
                file.seekg(0, std::ios::end);
                size_t total = file.tellg() / sizeof(uint8_t);
                if (inPrecision == Precision::FP32)
                    total = file.tellg() / sizeof(float);
                if (total != totalSize) {
                    // number of entries doesn't match, either not U8 or from different network
                    throw std::logic_error("Input contains " + std::to_string(total) + " entries, " +
                        "which doesn't match expected dimensions: " + std::to_string(totalSize));
                }
                file.seekg(0, std::ios::beg);
                inputSeq_u8.resize(total);
                file.read(reinterpret_cast<char *>(&inputSeq_u8[0]), total * sizeof(uint8_t));
                inputChannelMajor = true;

            } else {
                // use a provided image
                slog::info << "Using provided image..." << slog::endl;
                FormatReader::ReaderPtr reader(FLAGS_i.c_str());
                if (reader.get() == nullptr) {
                    throw std::logic_error("Image: " + FLAGS_i + " cannot be read!");
                }

                /** Store image data **/
                std::shared_ptr<unsigned char> data(
                        reader->getData(inputInfoItem.second->getTensorDesc().getDims()[3],
                                        inputInfoItem.second->getTensorDesc().getDims()[2]));
                if (data != nullptr) {
                    imagesData.push_back(data);
                }
                // store input in vector for processing later
                for (size_t i=0; i< totalSize; ++i) {
                    // std::cout << static_cast<unsigned int>(imagesData.at(0).get()[i]) << " | ";
                    inputSeq_u8.push_back(static_cast<uint8_t>(imagesData.at(0).get()[i]));
                }
            }
            validImageNames.push_back(FLAGS_i);
        }
        
        // Output precision
        OutputsDataMap outputInfo(network.getOutputsInfo());
        if (!FLAGS_op.empty()) {
            Precision prc = Precision::U8;
            if (FLAGS_op == "FP16") prc = Precision::FP16;
            else if (FLAGS_op == "FP32") prc = Precision::FP32;
            else prc = Precision::U8;

            // possibly multiple outputs
            for (auto outputInfoIt=outputInfo.begin(); outputInfoIt!=outputInfo.end(); ++outputInfoIt){
                outputInfoIt->second->setPrecision(prc);
                if (outputInfoIt->second->getDims().size() == 2) {
                    outputInfoIt->second->setLayout(InferenceEngine::Layout::NC);
                } else {
                    outputInfoIt->second->setLayout(InferenceEngine::Layout::NHWC);
                }
            }
        }

        /** Setting batch size using image count **/
        network.setBatchSize(1);
        size_t batchSize = network.getBatchSize();
        slog::info << "Batch size is " << std::to_string(batchSize) << slog::endl;

        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 4. Loading model to the device ------------------------------------------
        slog::info << "Loading model to the device" << slog::endl;
        ExecutableNetwork executable_network = ie.LoadNetwork(network, FLAGS_d);
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 5. Create infer request -------------------------------------------------
        slog::info << "Create infer request" << slog::endl;
        InferRequest inferRequest = executable_network.CreateInferRequest();
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 6. Prepare input --------------------------------------------------------
        for (auto & item : inputInfo) {
            Blob::Ptr inputBlob = inferRequest.GetBlob(item.first);
            SizeVector dims = inputBlob->getTensorDesc().getDims();
            /** Fill input tensor with images. First b channel, then g and r channels **/
            size_t num_channels = dims[1];
            size_t image_size = dims[3] * dims[2];
            // ASSUMPTION inputSeq_u8 is Z-Major (NHWC), BGR
            std::vector<uint8_t> input_nchw_rgb(num_channels * image_size);
            std::vector<uint8_t> input_nchw_bgr(num_channels * image_size);
            std::vector<uint8_t> input_nhwc_rgb(num_channels * image_size);
            std::vector<uint8_t> input_nhwc_bgr(num_channels * image_size);

            if (inPrecision == Precision::U8) {
                auto data = inputBlob->buffer().as<PrecisionTrait<Precision::U8>::value_type *>();
                if(inputChannelMajor) // Input is Channel Major, BGR
                {
                    for (size_t i = 0; i < inputSeq_u8.size(); ++i){
                        if(!FLAGS_r) // Keep Channel Major BGR
                            data[i] = inputSeq_u8[i];
                        input_nchw_bgr[i] = inputSeq_u8[i];
                        // Channel Major RGB
                        if(i < image_size) { // B
                            if(FLAGS_r)
                                data[i] = inputSeq_u8.at(i+(image_size*2));
                            input_nchw_rgb[i] = inputSeq_u8.at(i+(image_size*2));
                        }
                        else if ( i > image_size * 2) { // R
                            if(FLAGS_r)
                                data[i+(image_size*2)] = inputSeq_u8.at(i);
                            input_nchw_rgb[i+(image_size*2)] = inputSeq_u8.at(i);
                        }
                        else { // G
                            if(FLAGS_r)
                                data[i] = inputSeq_u8.at(i);
                            input_nchw_rgb[i] = inputSeq_u8.at(i);
                        }
                    }
                    //Create Z-Major BGR for KMB (may be required later)
                    for (size_t ch = 0; ch < num_channels; ++ch)
                    {
                        for (size_t pid = 0; pid < image_size; pid++)
                        {
                            input_nhwc_bgr[pid*num_channels + pid] = inputSeq_u8.at(ch*image_size + pid);
                        }
                    }

                    // Create Z-Major RGB. Use Z-Major BGR, just swap channels into RGB
                    for (size_t i = 1; i <  input_nhwc_bgr.size(); i=i+3)
                    {
                        input_nhwc_rgb[i-1] =  input_nhwc_bgr.at(i+1); // R <- B
                        input_nhwc_rgb[i] =  input_nhwc_bgr.at(i); // G
                        input_nhwc_rgb[i+1] = input_nhwc_bgr.at(i-1); // B <- R
                    }
                }
                else // Input is Z-Major, BGR. From bmp image, for example.
                {
                    for(size_t pid = 0; pid < inputSeq_u8.size(); pid++)
                        input_nhwc_bgr[pid] = inputSeq_u8[pid];
                    for (size_t pid = 0; pid < image_size; pid++) {
                        /** Iterate over all channels to create channel major input **/
                        for (size_t ch = 0; ch < num_channels; ++ch) {
                            int swap_ch = (num_channels-1) - ch;
                            if(!FLAGS_r) // Create channel major, BGR input for CPU Plugin
                                data[ch * image_size + pid] = inputSeq_u8.at(pid*num_channels + ch);
                            input_nchw_bgr[ch * image_size + pid] = inputSeq_u8.at(pid*num_channels + ch);
                            if(FLAGS_r) // Create channel major, RGB input for CPU Plugin
                                data[swap_ch * image_size + pid] = inputSeq_u8.at(pid*num_channels + ch);
                            input_nchw_rgb[swap_ch * image_size + pid] = inputSeq_u8.at(pid*num_channels + ch);
                        }
                    }
                    // Keep Z-major, just swap channels into RGB
                    for (size_t i = 1; i < inputSeq_u8.size(); i=i+3) {
                        input_nhwc_rgb[i-1] = inputSeq_u8.at(i+1); // R <- B
                        input_nhwc_rgb[i] = inputSeq_u8.at(i); // G
                        input_nhwc_rgb[i+1] = inputSeq_u8.at(i-1); // B <- R
                    }
                }
                writeToFile<uint8_t>(input_nhwc_bgr, "./input_cpu_nhwc_bgr.bin");
                writeToFile<uint8_t>(input_nhwc_rgb, "./input_cpu_nhwc_rgb.bin");
                writeToFile<uint8_t>(input_nchw_rgb, "./input_cpu_nchw_rgb.bin");
                writeToFile<uint8_t>(input_nchw_bgr, "./input_cpu_nchw_bgr.bin");
             }
            // TODO Update to create all 4 options for input
            else if (inPrecision == Precision::FP32) {
                std::ifstream file(FLAGS_i, std::ios::in | std::ios::binary);
                if (!file.is_open())
                    throw std::logic_error("Input: " + FLAGS_i + " cannot be read!");
                file.seekg(0, std::ios::end);
                size_t total = file.tellg() / sizeof(float);
                if (total != totalSize) {
                    // number of entries doesn't match, either not FP32 or from different network
                    throw std::logic_error("Input contains " + std::to_string(total) + " entries, " +
                                           "which doesn't match expected dimensions: " + std::to_string(totalSize));
                }
                file.seekg(0, std::ios::beg);
                inputSeq_fp32.resize(total);
                file.read(reinterpret_cast<char *>(&inputSeq_fp32[0]), total * sizeof(float));
                auto data = inputBlob->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();
                for (size_t pid = 0; pid < image_size; pid++) {
                    /** Iterate over all channels **/
                    for (size_t ch = 0; ch < num_channels; ++ch) {
                        data[ch * image_size + pid] = inputSeq_fp32.at(pid*num_channels + ch);
                        // std::cout << " data[" << (ch * image_size + pid) << "] = " << data[ch * image_size + pid];
                    }
                }
                writeToFile<float>(inputSeq_fp32, "./input_cpu_nhwc_bgr.bin");
                writeToFile<float>(inputSeq_fp32, "./input_cpu_nhwc_rgb.bin");
                writeToFile<float>(inputSeq_fp32, "./input_cpu_nchw_bgr.bin");
                writeToFile<float>(inputSeq_fp32, "./input_cpu_nchw_rgb.bin");
            }
        }

        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 7. Do inference ---------------------------------------------------------
        size_t numIterations = 1;
        size_t curIteration = 0;
        std::condition_variable condVar;

        inferRequest.SetCompletionCallback(
                [&] {
                    curIteration++;
                    slog::info << "Completed " << curIteration << " async request execution" << slog::endl;
                    if (curIteration < numIterations) {
                        /* here a user can read output containing inference results and put new input
                           to repeat async request again */
                        inferRequest.StartAsync();
                    } else {
                        /* continue sample execution after last Asynchronous inference request execution */
                        condVar.notify_one();
                    }
                });

        /* Start async request for the first time */
        slog::info << "Start inference (" << numIterations << " asynchronous executions)" << slog::endl;
        inferRequest.StartAsync();

        /* Wait all repetitions of the async request */
        std::mutex mutex;
        std::unique_lock<std::mutex> lock(mutex);
        condVar.wait(lock, [&]{ return curIteration == numIterations; });

        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 8. Process output -------------------------------------------------------
        slog::info << "Processing " << outputInfo.size() << " output blob" << ((outputInfo.size() > 1) ? "s" : "") << slog::endl;

        /** Read labels from file (e.x. AlexNet.labels) **/
        std::string labelFileName = fileNameNoExt(FLAGS_m) + ".labels";
        std::vector<std::string> labels;

        std::ifstream inputFile;
        inputFile.open(labelFileName, std::ios::in);
        if (inputFile.is_open()) {
            std::string strLine;
            while (std::getline(inputFile, strLine)) {
                trim(strLine);
                labels.push_back(strLine);
            }
        }

        std::vector<Blob::Ptr> outputBlob;
        for (auto outputInfoIt=outputInfo.begin(); outputInfoIt!=outputInfo.end(); ++outputInfoIt){
            outputBlob.push_back(inferRequest.GetBlob(outputInfoIt->first));
        }

        for (unsigned i = 0 ; i < outputBlob.size(); i++)
        {
            /** Validating -nt value **/
            const size_t resultsCnt = outputBlob[i]->size() / batchSize;
            if (FLAGS_nt > resultsCnt || FLAGS_nt < 1) {
                slog::warn << "-nt " << FLAGS_nt << " is not available for this network (-nt should be less than " \
                          << resultsCnt+1 << " and more than 0)\n            will be used maximal value : " << resultsCnt << slog::endl;
                FLAGS_nt = resultsCnt;
            }

            // save the results file for validation
            if (i == 0) dumpBlob(outputBlob[i], "./output_cpu.bin"); //TODO: remove when validator updated
            dumpBlob(outputBlob[i], "./output_cpu" + std::to_string(i) + ".bin");

            ClassificationResult classificationResult(outputBlob[i], validImageNames,
                                                      batchSize, FLAGS_nt,
                                                      labels);
            if (outputBlob.size() > 1)
                std::cout << "Output " << i << ":" << std::endl;
            classificationResult.print();

            std::string results_filename;
            if (outputBlob.size() == 1)
                results_filename = "./inference_results.txt";
            else
                results_filename = "./inference_results" + std::to_string(i) + ".txt";
            std::ofstream f(results_filename);
            auto topK = classificationResult.getResults();
            for(auto i = topK.begin(); i != topK.end(); ++i) {
                f << *i << '\n';
            }

        }


        // -----------------------------------------------------------------------------------------------------
    }
    catch (const std::exception& error) {
        slog::err << error.what() << slog::endl;
        return 1;
    }
    catch (...) {
        slog::err << "Unknown/internal exception happened." << slog::endl;
        return 1;
    }

    slog::info << "Execution successful" << slog::endl;
    slog::info << slog::endl << "This sample is an API example, for any performance measurements "
                                "please use the dedicated benchmark_app tool" << slog::endl;
    return 0;
}

